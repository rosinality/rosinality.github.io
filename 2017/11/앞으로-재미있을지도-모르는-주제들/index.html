<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.73.0"><meta name=theme content="Tranquilpeak 0.3.1-BETA"><title>앞으로 재미있을지도 모르는 주제들</title><meta name=author content="Kim Seonghyeon"><meta name=keywords content="deep learning,machine learning,machine learning,deep learning,social science"><link rel=icon href=/favicon.png><meta name=description content="LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거"><meta property="og:description" content="LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거"><meta property="og:type" content="blog"><meta property="og:title" content="앞으로 재미있을지도 모르는 주제들"><meta property="og:url" content="/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/"><meta property="og:site_name" content="Nondifferentiable Log"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nondifferentiable Log"><meta name=twitter:description content="LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거"><meta name=twitter:creator content="@rosinality"><meta property="og:image" content="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=640"><meta property="og:image" content="https://res.cloudinary.com/rosinality/image/upload/v1509636468/candle_kgokhm.jpg"><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css><link rel=stylesheet href=/css/style-u6mk0ojoywresbx8iepslrmmhl4stuhrsxuwhkpwrkrx7mryjcaimasnk4pi.min.css></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=/>Nondifferentiable Log</a></div></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=/#about><img class=sidebar-profile-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt></a><h4 class=sidebar-profile-name>Kim Seonghyeon</h4><h5 class=sidebar-profile-bio>Machine learning enthusiast</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/categories><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/tags><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tags</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/archives><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/#about><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>About</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Home</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/rosinality target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div class="post-header-cover
text-left
post-header-cover--partial" style=background-image:url(https://res.cloudinary.com/rosinality/image/upload/v1509636468/candle_kgokhm.jpg) data-behavior=5></div><div id=main data-behavior=5 class="hasCover
hasCoverMetaIn"><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-left"><h1 class=post-title itemprop=headline>앞으로 재미있을지도 모르는 주제들</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-11-03T10:03:59+09:00>November 3, 2017</time>
<span></span><a class=category-link href=/categories/thoughts>thoughts</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거나 어려웠던 문제를 해결하기는 어렵다. 성능을 개선하는 정도라고 볼 수 있다.</p><p>조금 더 재미있는 것은 어렵거나 불가능했던 문제를 해결할 수 있는 구조나 알고리즘을 설계하는 것이다. 아니면 지금까지는 생각하지 못했던 새로운 문제라거나. 물론 그게 쉬운 일은 아니다.</p><p>그렇다면 풀 수 없었던 문제를 풀기 위해선 무엇이 필요할까?</p><p>한 가지 재미있는 접근들은 뉴럴 네트워크의 가중치를 생성하는 모형이나 알고리즘들이다. HyperNetworks<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>에서부터 그 베이지안 확장인 Bayesian HyperNetworks<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> 같은 것이 그 대표적인 사례이며 Conditional Batch Normalization<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> 같은 것도 비슷한 경우라고 볼 수 있다. 이러한 알고리즘들은 뉴럴 네트워크를 유연하게 조절할 수 있는 기능을 제공한다는 방안을 제공한다는 것도 중요하지만 뉴럴 네트워크들의 공간을 이해할 수 있는 방향이라는 것이 또한 흥미롭다. 관련해서 재미있는 점 중 하나는 뉴럴 네트워크라는 하나의 함수가 가우시안 프로세스의 샘플로 이해할 수 있다는 사실이다.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><p>다른 한 가지 방향은 뉴럴 네트워크에 장착할 수 있는 새로운 부품을 만드는 것이다. 예컨대 주의Attention나 외부 메모리External Memory 같은 구조가 그 대표적인 사례라고 할 수 있다. 이런 구조들은 단순히 기존 모델의 부품을 교체해 성능을 향상시키는 것을 넘어 아주 새로운 문제를 풀 수 있게 해준다. 정말로 획기적인 구조는 기존의 방식으로는 학습이 어려울 수도 있고, 따라서 학습 알고리즘도 중요한 문제가 된다.</p><p>마지막 방향은 우리가 풀고자 하는 문제와 데이터를 더 잘 이해하게 되는 것이다. LSTM은 장기적 관계를 모델링하는 과제에서 지속적으로 문제를 겪고 있다. 이게 우리가 LSTM으로 학습하려고 하는 데이터의 구조에 의해서 발생하는 문제는 아닐까? p(x1, x2, x3) = p(x1)p(x2|x1)p(x3|x1, x2)와 같이 chain rule로 분해한 방식으로 언어에 대한 생성 모델을 정의하고 학습하는 것이 적절한 문제일까? 이 방향으로도 더 많은 고찰이 필요할 것이다.</p><p>아마도 이런 문제들에 대해 앞으로 더 중요해질 수학적 도구는 정보이론과 기하학이지 않을까 생각한다. 데이터와 모델이라는 관계를 정보라는 관점으로 생각하면 정보이론의 문제가 되고 곡률이라는 관점으로 생각하면 기하학의 문제가 되니까. 같은 문제에 대한 서로 다른 관점이지만, 같은 문제이기에 서로의 도구를 쓸 수 있다.</p><p>물론 앞으로 어떤 수학이 유용한 도구로 부상할지는 알 수 없는 것이지만.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Ha, D., Dai, A., & Le, Q. V. (2016). HyperNetworks. arXiv preprint. [https://arxiv.org/abs/1609.09106] <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Krueger, D., Huang, C. W., Islam, R., Turner, R., Lacoste, A., & Courville, A. (2017). Bayesian Hypernetworks. arXiv preprint. [https://arxiv.org/abs/1710.04759] <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>de Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O., & Courville, A. (2017). Modulating early visual processing by language. arXiv preprint. [https://arxiv.org/abs/1707.00683] <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., Sohl-Dickstein, J. (2017) Deep Neural Networks as Gaussian Processes. arXiv preprint. [https://arxiv.org/abs/1711.00165] <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small"></span><br><a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/deep-learning/>deep learning</a>
<a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/machine-learning/>machine learning</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=/2018/05/%EB%B2%84%EB%8B%9D/ data-tooltip=버닝><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml"></span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=/2017/06/%EC%98%A5%EC%9E%90/ data-tooltip=옥자><span class="hide-xs hide-sm text-small icon-mr"></span><i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=http%3a%2f%2frosinality.github.io%2f2017%2f11%2f%25EC%2595%259E%25EC%259C%25BC%25EB%25A1%259C-%25EC%259E%25AC%25EB%25AF%25B8%25EC%259E%2588%25EC%259D%2584%25EC%25A7%2580%25EB%258F%2584-%25EB%25AA%25A8%25EB%25A5%25B4%25EB%258A%2594-%25EC%25A3%25BC%25EC%25A0%259C%25EB%2593%25A4%2f"><i class="fa fa-google-plus"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frosinality.github.io%2f2017%2f11%2f%25EC%2595%259E%25EC%259C%25BC%25EB%25A1%259C-%25EC%259E%25AC%25EB%25AF%25B8%25EC%259E%2588%25EC%259D%2584%25EC%25A7%2580%25EB%258F%2584-%25EB%25AA%25A8%25EB%25A5%25B4%25EB%258A%2594-%25EC%25A3%25BC%25EC%25A0%259C%25EB%2593%25A4%2f"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=http%3a%2f%2frosinality.github.io%2f2017%2f11%2f%25EC%2595%259E%25EC%259C%25BC%25EB%25A1%259C-%25EC%259E%25AC%25EB%25AF%25B8%25EC%259E%2588%25EC%259D%2584%25EC%25A7%2580%25EB%258F%2584-%25EB%25AA%25A8%25EB%25A5%25B4%25EB%258A%2594-%25EC%25A3%25BC%25EC%25A0%259C%25EB%2593%25A4%2f"><i class="fa fa-twitter"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><span class=copyrights>&copy; 2024 Kim Seonghyeon.</span></footer></div><div id=share-options-bar class=share-options-bar data-behavior=5><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=http%3a%2f%2frosinality.github.io%2f2017%2f11%2f%25EC%2595%259E%25EC%259C%25BC%25EB%25A1%259C-%25EC%259E%25AC%25EB%25AF%25B8%25EC%259E%2588%25EC%259D%2584%25EC%25A7%2580%25EB%258F%2584-%25EB%25AA%25A8%25EB%25A5%25B4%25EB%258A%2594-%25EC%25A3%25BC%25EC%25A0%259C%25EB%2593%25A4%2f"><i class="fa fa-google-plus"></i><span></span></a></li><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frosinality.github.io%2f2017%2f11%2f%25EC%2595%259E%25EC%259C%25BC%25EB%25A1%259C-%25EC%259E%25AC%25EB%25AF%25B8%25EC%259E%2588%25EC%259D%2584%25EC%25A7%2580%25EB%258F%2584-%25EB%25AA%25A8%25EB%25A5%25B4%25EB%258A%2594-%25EC%25A3%25BC%25EC%25A0%259C%25EB%2593%25A4%2f"><i class="fa fa-facebook-official"></i><span></span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=http%3a%2f%2frosinality.github.io%2f2017%2f11%2f%25EC%2595%259E%25EC%259C%25BC%25EB%25A1%259C-%25EC%259E%25AC%25EB%25AF%25B8%25EC%259E%2588%25EC%259D%2584%25EC%25A7%2580%25EB%258F%2584-%25EB%25AA%25A8%25EB%25A5%25B4%25EB%258A%2594-%25EC%25A3%25BC%25EC%25A0%259C%25EB%2593%25A4%2f"><i class="fa fa-twitter"></i><span></span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt><h4 id=about-card-name>Kim Seonghyeon</h4><div id=about-card-bio>Machine learning enthusiast</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Graduate student in HCCLab at Seoul National University</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Korea, Republic of</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input type=text id=algolia-search-input name=search class="form-control input--large search-input" placeholder></form></div><div class=modal-body><div class="no-result text-color-light text-center"></div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/post/><h3 class=media-heading>Posts</h3></a><span class=media-meta><span class="media-date text-small">Sep 9, 2024</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/><h3 class=media-heading>Scaling Law, Architecture for Stability and Layer Stacking</h3></a><span class=media-meta><span class="media-date text-small">Sep 9, 2024</span></span><div class="media-content hide-xs font-merryweather">Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/><h3 class=media-heading>Preliminary Explorations on UL2 and Second-order Optimizers</h3></a><span class=media-meta><span class="media-date text-small">Jun 6, 2024</span></span><div class="media-content hide-xs font-merryweather">In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/07/constitutional-ai/><h3 class=media-heading>Constitutional AI</h3></a><span class=media-meta><span class="media-date text-small">Jul 7, 2023</span></span><div class="media-content hide-xs font-merryweather">Helpful & Harmless Agent AI 모델의 정렬(Alignment)이라고 이야기할 때 흔히 나오는 Helpfulness와 Harmlessness는 어떤 의미인가? 이는 정의</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%99%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4/><h3 class=media-heading>이미지와 텍스트 생성 모델에 대해</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">이미지 생성 하면 Style GAN이었던 시절에도 일러스트 생성 등은 오타쿠적 인기가 있는 주제였다. 문제의 Danbooru 데이터셋 같은 경우에도 그 시점에 이미 만들어진 데이터셋이었</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%96%B8%EC%96%B4%EC%9D%98-%EC%86%90%EC%8B%A4-%EC%95%95%EC%B6%95%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/><h3 class=media-heading>언어의 손실 압축에 대하여</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web LM을 다음 단어를 예측할 뿐이라거나 학습 데이터를 기억할 뿐이라는 식으로 묘사하는 것은 폄하를 위한 언어이지 LM의 실체나 실제 한계에 대해서 논하기에 적절한 방</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/01/ocr-%ED%9A%8C%EA%B3%A0/><h3 class=media-heading>OCR 회고</h3></a><span class=media-meta><span class="media-date text-small">Jan 1, 2023</span></span><div class="media-content hide-xs font-merryweather">타이틀 커버 이미지 출처: https://www.behance.net/gallery/6146939/OCR-A-Poster/modules/152114859 4년 동안 몰두했던 OCR이라는 주제를 마무리하게 되면서 으레 그래왔듯 회고를 남겨본다. 이랬더라면 어땠을까 같은 소소한 소회보다는</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-7/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 7</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-6/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 6</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-5/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 5</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero data-message-one data-message-other>68 posts found</p></div></div></div><div id=cover style=background-image:url(http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/stair.jpg)></div><script>(function(d){var config={kitId:'vld6lxf',scriptTimeout:3000,async:true},h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)})(document);</script><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js></script><script src=/js/script-wl33z0n6ocaypepiqrazthtivfrliqijej4rq8ek8gvrv1awftmgjuv8k4zc.min.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight').each(function(i,block){var code="";hljs.highlightAuto(block.innerText).value.split(/\r\n|\r|\n/).forEach(function(line){code+="<span class=\"line\">"+line+"</span><br>";});if(code.length>0){block.innerHTML=code;}});$('pre > code').each(function(i,block){$(this).addClass('codeblock');hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='http:\/\/rosinality.github.io\/2017\/11\/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4\/';this.page.identifier='\/2017\/11\/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='rosinality';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script></body></html>