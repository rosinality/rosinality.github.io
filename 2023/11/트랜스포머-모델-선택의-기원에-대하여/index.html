<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.73.0"><meta name=theme content="Tranquilpeak 0.3.1-BETA"><title>트랜스포머 모델 선택의 기원에 대하여</title><meta name=author content="Kim Seonghyeon"><meta name=keywords content="machine learning,machine learning,deep learning,social science"><link rel=icon href=/favicon.png><meta name=description content="트랜스포머는 이제 너무나 유명한 모델이고 아마 딥 러닝 모델 중에서 트랜스포머보다 많은 설명이 작성된 모델 또한 없을 것 같다. 그런 상황에서 트랜스포머의 디자인에 대"><meta property="og:description" content="트랜스포머는 이제 너무나 유명한 모델이고 아마 딥 러닝 모델 중에서 트랜스포머보다 많은 설명이 작성된 모델 또한 없을 것 같다. 그런 상황에서 트랜스포머의 디자인에 대"><meta property="og:type" content="blog"><meta property="og:title" content="트랜스포머 모델 선택의 기원에 대하여"><meta property="og:url" content="/2023/11/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D%EC%9D%98-%EA%B8%B0%EC%9B%90%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/"><meta property="og:site_name" content="Nondifferentiable Log"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nondifferentiable Log"><meta name=twitter:description content="트랜스포머는 이제 너무나 유명한 모델이고 아마 딥 러닝 모델 중에서 트랜스포머보다 많은 설명이 작성된 모델 또한 없을 것 같다. 그런 상황에서 트랜스포머의 디자인에 대"><meta name=twitter:creator content="@rosinality"><meta property="og:image" content="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=640"><meta property="og:image" content="https://res.cloudinary.com/rosinality/image/upload/f_auto,q_auto/v1/covers/transformer"><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css><link rel=stylesheet href=/css/style-u6mk0ojoywresbx8iepslrmmhl4stuhrsxuwhkpwrkrx7mryjcaimasnk4pi.min.css></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=/>Nondifferentiable Log</a></div></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=/#about><img class=sidebar-profile-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt></a><h4 class=sidebar-profile-name>Kim Seonghyeon</h4><h5 class=sidebar-profile-bio>Machine learning enthusiast</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/categories><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/tags><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tags</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/archives><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/#about><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>About</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Home</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/rosinality target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div class="post-header-cover
text-left
post-header-cover--partial" style=background-image:url(https://res.cloudinary.com/rosinality/image/upload/f_auto,q_auto/v1/covers/transformer) data-behavior=5></div><div id=main data-behavior=5 class="hasCover
hasCoverMetaIn"><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-left"><h1 class=post-title itemprop=headline>트랜스포머 모델 선택의 기원에 대하여</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2023-11-03T18:27:16+09:00>November 3, 2023</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta informative</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>트랜스포머는 이제 너무나 유명한 모델이고 아마 딥 러닝 모델 중에서 트랜스포머보다 많은 설명이 작성된 모델 또한 없을 것 같다. 그런 상황에서 트랜스포머의 디자인에 대한 글을 또 쓰는 것이 의미가 있을까?</p><p>그렇지만 조금 다르게 트랜스포머라는 모델의 디자인적 선택의 기원과 유래, 혹은 반드시 그 아이디어에서 차용한 것은 아니더라도 유사함을 발견할 수 있는 과거의 사례들을 짚어보면 재미있지 않을까 싶었다. 트랜스포머도 이제 발표된지 6년이 넘어가는 오래된 모델이니 디자인적 유래를 소개하는 것은 그보다 더 오래된, 딥 러닝 판에서는 까마득히 오래된 시점의 일들을 소개하는 것이 될 듯 싶다. 그러니 옛 이야기를 듣는 것처럼 생각할 수도 있지 않을까.</p><p>주의사항 한 가지. 여기서 소개하는 기원이나 이전 아이디어 같은 경우는 대부분 내 기억과 익숙함에 의존하는 것이기에 실제로 최초라고 보장할 수는 없다. 사실 최초의 것을 찾아내려고 노력했다기보다는 오히려 트랜스포머와 보다 인접한 시점 혹은 더 유사하거나 가까운 것을 골랐다고 하는 것이 맞을 듯 싶다.</p><p>단적으로 말하면 트랜스포머 디자인과 관련된 그저 하고 싶은 이야기를 하려는 것이다.</p><h1 id=self-attention>Self Attention</h1><p>Hard attention<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>의 사례를 먼저 가져올 수도 있겠지만 softmax를 사용하는 일반적인 soft attention에 대해서는 Bahdanau<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>의 전설적인 논문에서 시작해도 큰 문제는 없을 듯 싶다. 이때는 $\operatorname{tanh}$를 activation으로 사용하는 작은 뉴럴 네트워크를 attention score로 사용했다. $\operatorname{softmax}(v\operatorname{tanh}(Wx))$ 같은 식으로. 트랜스포머에서처럼 dot product를 사례로는 Luong<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>의 논문을 들 수 있을 것 같다. 사실 여기에는 dot product 뿐만 아니라 attention의 여러 가능한 디자인에 대한 ablation이 있다. RNN 시절이기도 해서 attention을 어느 위치에서 계산해야 하는가 하는 것도 중요한 디자인적 선택이었다.</p><p>그렇다면 self attention의 시작은 무엇일까? Attention Is All You Need에서 인용하고 있는 것처럼, 트랜스포머가 등장하기 직전에 self attention이라고 볼 수 있는 동일 시퀀스 내에서 attention을 계산하는 방법들이 등장하기 시작했다. 이 때의 아이디어는 LSTM이 시퀀스를 고정된 크기의 상태 벡터로 압축해나가면서 먼 거리에 있는 단어들을 적절하게 다루거나 반영하는 것이 어려울 수 있다, 즉 원거리에 있는 단어들 사이의 관계를 모델링하기 어려울 수 있다는 아이디어에서 시작했다. 인코더 RNN으로 입력 시퀀스를 압축해서 나온 상태 벡터 하나만으로는 긴 텍스트를 다루는 것이 어렵다는 아이디어에서 attention이 시작되었다는 것을 생각하면 인코더와 디코더 사이 뿐만 아니라 인코더나 디코더 내에서도 그 문제를 해소할 필요가 있다는 아이디어로 이어졌다고 볼 수도 있겠다.</p><p>어쨌든 여기까지는 attention만으로도 충분할 수 있다는 과감함까지는 이르지 못했고 LSTM을 attention이 적절히 보강해줄 수 있을 것이라는 단계의 결과였다.</p><h1 id=multi-head-attention>Multi-Head Attention</h1><p>트랜스포머 self attention의 주요한 특징은 dot product attention이라는 것과 함께 여러 attention (head)를 결합한 attention이라는 것일 것이다. attention 하나가 아니라 여러 attention을 결합하자는 아이디어는 A Structured Self-Attentive Sentence Embedding<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>에서 유사한 시도를 찾아볼 수 있을 듯 싶다. 트랜스포머에서처럼 보통 attention은 특정한 부분, 다시 말해 토큰을 가져오도록 학습이 되므로 여러 토큰을 가져와서 결합할 수 있으려면 여러 개의 attention이 필요할 수 있다는 아이디어였다.</p><h1 id=skip-connection>Skip Connection</h1><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Mnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. Advances in neural information processing systems, 27. <a href=https://arxiv.org/abs/1406.6247>https://arxiv.org/abs/1406.6247</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. <a href=https://arxiv.org/abs/1409.0473>https://arxiv.org/abs/1409.0473</a> <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025. <a href=https://arxiv.org/abs/1508.04025>https://arxiv.org/abs/1508.04025</a> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Lin, Z., Feng, M., Santos, C. N. D., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130. <a href=https://arxiv.org/abs/1703.03130>https://arxiv.org/abs/1703.03130</a> <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small"></span><br><a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/machine-learning/>machine learning</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=/2023/11/%EA%B7%B8%EB%8C%80%EB%93%A4%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%B4-%EA%B2%83%EC%9D%B8%EA%B0%80/ data-tooltip="그대들은 어떻게 살 것인가"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml"></span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=/2023/07/constitutional-ai/ data-tooltip="Constitutional AI"><span class="hide-xs hide-sm text-small icon-mr"></span><i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=http%3a%2f%2frosinality.github.io%2f2023%2f11%2f%25ED%258A%25B8%25EB%259E%259C%25EC%258A%25A4%25ED%258F%25AC%25EB%25A8%25B8-%25EB%25AA%25A8%25EB%258D%25B8-%25EC%2584%25A0%25ED%2583%259D%25EC%259D%2598-%25EA%25B8%25B0%25EC%259B%2590%25EC%2597%2590-%25EB%258C%2580%25ED%2595%2598%25EC%2597%25AC%2f"><i class="fa fa-google-plus"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frosinality.github.io%2f2023%2f11%2f%25ED%258A%25B8%25EB%259E%259C%25EC%258A%25A4%25ED%258F%25AC%25EB%25A8%25B8-%25EB%25AA%25A8%25EB%258D%25B8-%25EC%2584%25A0%25ED%2583%259D%25EC%259D%2598-%25EA%25B8%25B0%25EC%259B%2590%25EC%2597%2590-%25EB%258C%2580%25ED%2595%2598%25EC%2597%25AC%2f"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=http%3a%2f%2frosinality.github.io%2f2023%2f11%2f%25ED%258A%25B8%25EB%259E%259C%25EC%258A%25A4%25ED%258F%25AC%25EB%25A8%25B8-%25EB%25AA%25A8%25EB%258D%25B8-%25EC%2584%25A0%25ED%2583%259D%25EC%259D%2598-%25EA%25B8%25B0%25EC%259B%2590%25EC%2597%2590-%25EB%258C%2580%25ED%2595%2598%25EC%2597%25AC%2f"><i class="fa fa-twitter"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><span class=copyrights>&copy; 2024 Kim Seonghyeon.</span></footer></div><div id=share-options-bar class=share-options-bar data-behavior=5><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=http%3a%2f%2frosinality.github.io%2f2023%2f11%2f%25ED%258A%25B8%25EB%259E%259C%25EC%258A%25A4%25ED%258F%25AC%25EB%25A8%25B8-%25EB%25AA%25A8%25EB%258D%25B8-%25EC%2584%25A0%25ED%2583%259D%25EC%259D%2598-%25EA%25B8%25B0%25EC%259B%2590%25EC%2597%2590-%25EB%258C%2580%25ED%2595%2598%25EC%2597%25AC%2f"><i class="fa fa-google-plus"></i><span></span></a></li><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frosinality.github.io%2f2023%2f11%2f%25ED%258A%25B8%25EB%259E%259C%25EC%258A%25A4%25ED%258F%25AC%25EB%25A8%25B8-%25EB%25AA%25A8%25EB%258D%25B8-%25EC%2584%25A0%25ED%2583%259D%25EC%259D%2598-%25EA%25B8%25B0%25EC%259B%2590%25EC%2597%2590-%25EB%258C%2580%25ED%2595%2598%25EC%2597%25AC%2f"><i class="fa fa-facebook-official"></i><span></span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=http%3a%2f%2frosinality.github.io%2f2023%2f11%2f%25ED%258A%25B8%25EB%259E%259C%25EC%258A%25A4%25ED%258F%25AC%25EB%25A8%25B8-%25EB%25AA%25A8%25EB%258D%25B8-%25EC%2584%25A0%25ED%2583%259D%25EC%259D%2598-%25EA%25B8%25B0%25EC%259B%2590%25EC%2597%2590-%25EB%258C%2580%25ED%2595%2598%25EC%2597%25AC%2f"><i class="fa fa-twitter"></i><span></span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt><h4 id=about-card-name>Kim Seonghyeon</h4><div id=about-card-bio>Machine learning enthusiast</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Graduate student in HCCLab at Seoul National University</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Korea, Republic of</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input type=text id=algolia-search-input name=search class="form-control input--large search-input" placeholder></form></div><div class=modal-body><div class="no-result text-color-light text-center"></div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/post/><h3 class=media-heading>Posts</h3></a><span class=media-meta><span class="media-date text-small">Jun 6, 2024</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/><h3 class=media-heading>Preliminary Explorations on UL2 and Second-order Optimizers</h3></a><span class=media-meta><span class="media-date text-small">Jun 6, 2024</span></span><div class="media-content hide-xs font-merryweather">In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/11/%EC%98%A4%ED%8E%9C%ED%95%98%EC%9D%B4%EB%A8%B8/><h3 class=media-heading>오펜하이머</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2023</span></span><div class="media-content hide-xs font-merryweather">영화의 가장 중요한 질문. 청문회에서 오펜하이머가 본 환영. 왜 지금 후회하고 반대할 일들을 그 당시에는 예측하지 못했는가? 아니 사실 알고 있지 않았는가? 그런데 알</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/11/%EA%B7%B8%EB%8C%80%EB%93%A4%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%B4-%EA%B2%83%EC%9D%B8%EA%B0%80/><h3 class=media-heading>그대들은 어떻게 살 것인가</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2023</span></span><div class="media-content hide-xs font-merryweather">이것은 상징이라고 이렇게까지 말하는 영화는 그리 많지 않을 듯 싶다. 상징이라는 것은 분명한데 이것이 무엇의 상징인지, 그리고 그 상징들이 모여서 어떤 한 가지를 말하</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/11/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D%EC%9D%98-%EA%B8%B0%EC%9B%90%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/><h3 class=media-heading>트랜스포머 모델 선택의 기원에 대하여</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2023</span></span><div class="media-content hide-xs font-merryweather">트랜스포머는 이제 너무나 유명한 모델이고 아마 딥 러닝 모델 중에서 트랜스포머보다 많은 설명이 작성된 모델 또한 없을 것 같다. 그런 상황에서 트랜스포머의 디자인에 대</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/07/constitutional-ai/><h3 class=media-heading>Constitutional AI</h3></a><span class=media-meta><span class="media-date text-small">Jul 7, 2023</span></span><div class="media-content hide-xs font-merryweather">Helpful & Harmless Agent AI 모델의 정렬(Alignment)이라고 이야기할 때 흔히 나오는 Helpfulness와 Harmlessness는 어떤 의미인가? 이는 정의</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%99%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4/><h3 class=media-heading>이미지와 텍스트 생성 모델에 대해</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">이미지 생성 하면 Style GAN이었던 시절에도 일러스트 생성 등은 오타쿠적 인기가 있는 주제였다. 문제의 Danbooru 데이터셋 같은 경우에도 그 시점에 이미 만들어진 데이터셋이었</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%96%B8%EC%96%B4%EC%9D%98-%EC%86%90%EC%8B%A4-%EC%95%95%EC%B6%95%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/><h3 class=media-heading>언어의 손실 압축에 대하여</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web LM을 다음 단어를 예측할 뿐이라거나 학습 데이터를 기억할 뿐이라는 식으로 묘사하는 것은 폄하를 위한 언어이지 LM의 실체나 실제 한계에 대해서 논하기에 적절한 방</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/01/ocr-%ED%9A%8C%EA%B3%A0/><h3 class=media-heading>OCR 회고</h3></a><span class=media-meta><span class="media-date text-small">Jan 1, 2023</span></span><div class="media-content hide-xs font-merryweather">타이틀 커버 이미지 출처: https://www.behance.net/gallery/6146939/OCR-A-Poster/modules/152114859 4년 동안 몰두했던 OCR이라는 주제를 마무리하게 되면서 으레 그래왔듯 회고를 남겨본다. 이랬더라면 어땠을까 같은 소소한 소회보다는</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-7/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 7</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero data-message-one data-message-other>70 posts found</p></div></div></div><div id=cover style=background-image:url(http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/stair.jpg)></div><script>(function(d){var config={kitId:'vld6lxf',scriptTimeout:3000,async:true},h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)})(document);</script><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js></script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    preview: "none",
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script><script src=/js/script-wl33z0n6ocaypepiqrazthtivfrliqijej4rq8ek8gvrv1awftmgjuv8k4zc.min.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight').each(function(i,block){var code="";hljs.highlightAuto(block.innerText).value.split(/\r\n|\r|\n/).forEach(function(line){code+="<span class=\"line\">"+line+"</span><br>";});if(code.length>0){block.innerHTML=code;}});$('pre > code').each(function(i,block){$(this).addClass('codeblock');hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='http:\/\/rosinality.github.io\/2023\/11\/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D%EC%9D%98-%EA%B8%B0%EC%9B%90%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC\/';this.page.identifier='\/2023\/11\/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D%EC%9D%98-%EA%B8%B0%EC%9B%90%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='rosinality';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script></body></html>