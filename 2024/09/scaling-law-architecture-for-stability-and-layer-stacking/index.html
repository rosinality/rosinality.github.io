<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.73.0"><meta name=theme content="Tranquilpeak 0.3.1-BETA"><title>Scaling Law, Architecture for Stability and Layer Stacking</title><meta name=author content="Kim Seonghyeon"><meta name=keywords content="machine learning,language model,scaling law,architecture,optimization,layer stacking,machine learning,deep learning,social science"><link rel=icon href=/favicon.png><meta name=description content="Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about"><meta property="og:description" content="Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about"><meta property="og:type" content="blog"><meta property="og:title" content="Scaling Law, Architecture for Stability and Layer Stacking"><meta property="og:url" content="/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/"><meta property="og:site_name" content="Nondifferentiable Log"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nondifferentiable Log"><meta name=twitter:description content="Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about"><meta name=twitter:creator content="@rosinality"><meta property="og:image" content="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=640"><meta property="og:image" content="https://res.cloudinary.com/rosinality/image/upload/c_pad,w_2048/scaling-law/ajyfaru0bjpaq5euvzxs"><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css><link rel=stylesheet href=/css/style-u6mk0ojoywresbx8iepslrmmhl4stuhrsxuwhkpwrkrx7mryjcaimasnk4pi.min.css></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=/>Nondifferentiable Log</a></div></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=/#about><img class=sidebar-profile-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt></a><h4 class=sidebar-profile-name>Kim Seonghyeon</h4><h5 class=sidebar-profile-bio>Machine learning enthusiast</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/categories><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/tags><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tags</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/archives><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/#about><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>About</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Home</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/rosinality target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div class="post-header-cover
text-left
post-header-cover--partial" style=background-image:url(https://res.cloudinary.com/rosinality/image/upload/c_pad,w_2048/scaling-law/ajyfaru0bjpaq5euvzxs) data-behavior=5></div><div id=main data-behavior=5 class="hasCover
hasCoverMetaIn"><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-left"><h1 class=post-title itemprop=headline>Scaling Law, Architecture for Stability and Layer Stacking</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2024-09-11T12:24:59+09:00>September 11, 2024</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta-informative</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><h2 id=scaling-law>Scaling Law</h2><p>Scaling law is one of the most important findings in LLMs (and neural networks in general) <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, hyperparameters such as learning rate and batch size <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, learning rate schedules <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, mixture of training datasets <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>, etc.</p><p>So if you are serious about builiding or understanding LLMs, you should be able to estimate the scaling law. And&mldr;I didn&rsquo;t have experiences about it. Anyway you need to have nontrivial amout of computing power because estimation of scaling law requires to train many models. But again TPU Research Cloud Program (<a href=https://sites.research.google/trc/about/>https://sites.research.google/trc/about/</a>) allowed me to get (at least for me) interesting results. I am deeply grateful to them.</p><h2 id=code-base>Code Base</h2><p>First I needed to get simple and hackable code base for training language models in jax. But also I should support at least FSDP to training interesting sized models. I have first considered MaxText (<a href=https://github.com/google/maxtext>https://github.com/google/maxtext</a>) but I found NanoDO (<a href=https://github.com/google-deepmind/nanodo>https://github.com/google-deepmind/nanodo</a>) is more simple.</p><p>NanoDO is minimal framework for training language models in jax with support of FSDP. I have modified it to support multi-host training. I have uploaded code that I have used for experiments.s (<a href=https://github.com/rosinality/nanodo>https://github.com/rosinality/nanodo</a>, messy!)</p><h3 id=caveats>Caveats</h3><p>NanoDO uses Grain (<a href=https://github.com/google/grain>https://github.com/google/grain</a>) for data loading. Grain is quite similar to PyTorch dataloader, which uses multiprocessing to load data in parallel and allows arbitrary python code to preprocess data. Grain depends on ArrayRecord (<a href=https://github.com/google/array_record>https://github.com/google/array_record</a>) which is a data format that allows random access. So it is quite trivial to restart training from previous checkpoint.</p><p>But due to support for random access, you cannot directly use GCP buckets. You should mount GCP bucket to your VM instance using gcsfuse, and you can refer to script from MaxText (<a href=https://github.com/google/maxtext/blob/main/setup_gcsfuse.sh)>https://github.com/google/maxtext/blob/main/setup_gcsfuse.sh)</a>. But&mldr;I found this is quite unstable, and other peoples also experienced this (<a href=https://github.com/google/maxtext/issues/786)>https://github.com/google/maxtext/issues/786)</a>. Any it works well after some warmup (?) period. I don&rsquo;t know how google deals with this.</p><p>Checkpointing using Orbax requires storage global, that is accessible from all hosts. Like GCP buckets. (<a href=https://github.com/google/orbax/issues/999>https://github.com/google/orbax/issues/999</a>) You can just use GCP bucket for this, but it is not documented well.</p><h2 id=experiment-settingsss>Experiment Settingsss</h2><p>I made my experiment settings with reference to Resolving Discrepancies in Compute-Optimal Scaling of Language Models <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. It allowed me to do manageable sized experiments for estimating scaling law. My settings is as follows.</p><table><thead><tr><th>Setting</th><th>Value</th></tr></thead><tbody><tr><td>Sequence Length</td><td>1024</td></tr><tr><td>Vocabulary Size</td><td>32101 (Llama 1)</td></tr><tr><td>Number of Heads</td><td>4 (Small, but same with <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.)</td></tr><tr><td>FFN Activation</td><td>SwiGLU</td></tr><tr><td>FFN Multiplier</td><td>3.5</td></tr><tr><td>Positional Embedding</td><td>RoPE</td></tr><tr><td>Independent Weight Decay</td><td>1e-4</td></tr><tr><td>Attention Logit Softcapping</td><td>50 (Adopted from Gemma 2 <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>)</td></tr><tr><td>Final Learning Rate Multiplier</td><td>0.1</td></tr><tr><td>Warmup Steps</td><td>Varying, with tokens same size with model parameters</td></tr><tr><td>Embedding Init</td><td>Normal(0.01)</td></tr><tr><td>Linear Init</td><td>Variance Scaling 1.0, fan-in</td></tr><tr><td>Residual Block Output Linear Init</td><td>Variance Scaling 1.0 / sqrt(number of layers), fan-in</td></tr><tr><td>LM Head Init</td><td>Variance Scaling 1.0, fan-in</td></tr><tr><td>Weight Tying</td><td>False</td></tr><tr><td>Normalization</td><td>RMSNorm</td></tr><tr><td>Training Dataset</td><td>FineWeb-Edu</td></tr></tbody></table><p>Final loss for each model is calculated using separate validation set. Model for each scale was like this</p><table><thead><tr><th>ID</th><th>Parameters</th><th>FLOPS/Token</th><th>Dimensions</th><th>Layers</th><th>Batch Size</th><th></th></tr></thead><tbody><tr><td>1</td><td>19M</td><td>86M</td><td>288</td><td>8</td><td>112</td><td></td></tr><tr><td>2</td><td>24M</td><td>116M</td><td>320</td><td>9</td><td>128</td><td></td></tr><tr><td>3</td><td>34M</td><td>175M</td><td>384</td><td>10</td><td>160</td><td></td></tr><tr><td>4</td><td>56M</td><td>311M</td><td>480</td><td>12</td><td>208</td><td></td></tr><tr><td>5</td><td>86M</td><td>503M</td><td>576</td><td>14</td><td>256</td><td></td></tr><tr><td>6</td><td>110M</td><td>652M</td><td>640</td><td>15</td><td>320</td><td></td></tr><tr><td>7</td><td>152M</td><td>932M</td><td>704</td><td>18</td><td>384</td><td></td></tr><tr><td>8</td><td>238M</td><td>1479M</td><td>832</td><td>21</td><td>512</td><td></td></tr><tr><td>9</td><td>383M</td><td>2388M</td><td>1024</td><td>23</td><td>640</td><td></td></tr><tr><td>10</td><td>509M</td><td>3195M</td><td>1120</td><td>26</td><td>896</td><td></td></tr><tr><td>11</td><td>691M</td><td>4313M</td><td>1312</td><td>26</td><td>1024</td><td></td></tr></tbody></table><p>In <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> it is recommended to consider include number of parameters of the embedding to the model. As I have used untied embeddings, I only counted number of parameters of embeddings, not LM head. (But anyway, for estimating scaling law I have used FLOPS/token instead of parameters.)</p><p>I have adopted batch sizes from <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, but as my sequence length is half, I have double the batch sizes. Instead of using learning rate scaling law, I have used same learning rate for all models with 3e-3 which I have searched with 34M model, but I have scaled learning rate of linear weights for another models with $384 / dim$. <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> I found this transfers with different model scales.</p><p>I have used AdamW with eta_1 = 0.9$, eta_2 = 0.98$. It was my mistake to use same eta_2$ for all scales, but I expect 0.98 is close enough to optimal for this model scales.</p><p>Models trained with same FLOPS budget, 1.25e17. For larger scale integer multiple of 1.25e17 is used.</p><h2 id=results>Results</h2><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726121687/scaling-law/isoflops.png alt="IsoFlops curve of scaling law experiment"></p><p>So after hyperparameter tuning and experiments I got this IsoFLOPS curve <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. I have measured FLOPS/Token instead of parameters, with reference to <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Anyway I have suprised that I could get a such nicely fitted curve.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726124030/scaling-law/optimal-loss.png alt="Loss of the model with optimal FLOPS/Token for each training FLOPS"></p><p>So by finding minimum of the IsoFLOPS curve for each training scales, I was able to get loss curve for each training scales given allocated FLOPS/Token is optimal.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726124030/scaling-law/optimal-flops.png alt="Optimal FLOPS/Token for each training FLOPS"></p><p>Then I drew for each training FLOPS, optimal allocation of FLOPS/Token of the model. Fitted function is $0.0149C^{0.58}$ and exponent 0.58 which is close to 0.578 that is estimated for OpenWebText2 in <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><h2 id=architectural-modifications-and-learning-rate-sensitivity>Architectural Modifications and Learning Rate Sensitivity</h2><p>During experimentation I have touched about learning rate transfer for efficiency. After the experiment, I thought might attention logit softcapping (and z-loss <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>) was vastly beneficial for learning rate transferability. It is known that explosion of attention logit induces training instabilities for higher leearning rates, and thus reduce transferability of learning rates. One of well-known solution for this phenomena is QK Norm which bounds maximum attention logits <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>. So maybe softcapping of attention logits could have similar effect, as it also bounds attention logits anyway.</p><p>QK Norm is trick of applying layer normalization (or RMSNorm) on query and key before computing attention logits, like $\operatorname{LN}(Q)\operatorname{LN}(K)^\intercal$. Attention logit softcapping is applying softclipping using tanh like $\alpha\operatorname{tanh}(\frac{1}{\alpha}QK^\intercal)$, where $\alpha$ is a constant like 50. Post normalization is applying layer normalization after output linear layer for each residual block, like $y = x + \operatorname{LN}(\operatorname{Block}(x))$. Post normalization was actually refering normalization setting of original transformer paper, $y = \operatorname{LN}(x + \operatorname{Block}(x))$ <sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>. But it is not popular these days due to training instabilities <sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> (it is commonly said that post normalization is better for performance, though <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>) and many people started refer $y = x + \operatorname{LN}(\operatorname{Block}(x))$ as a post normalization <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>.</p><p>Therefore I started learning rate sensitivity experiments, like <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>. The main focus of <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> was the change of the loss given change of the learning rate, and measuring width of basin of learning rates that induces similar loss. Slightly different from this, I want to know that architectural modifications like QK Norm or Softcapping can allow learning rate transfer. Actually in <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> authors have shown that with QK Norm it is possible to use same learning rate 1e-3 for ViT-L (307M) and ViT-22B (of course, 22B)! It is more than 70x increase in model sizes. (Also, authors reported that it is not trainable in that scale without QK Norm. <a href=https://x.com/jmgilmer/status/1626276273168470016>https://x.com/jmgilmer/status/1626276273168470016</a>)</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726127387/scaling-law/lr-sensitivity.png alt="Learning rate sensitivity (LR vs Loss) for architectural modifications, for 34M and 691M models"></p><p>I have trained 34M and 691M models with fixed 50K steps and 5,000 warmup steps. I have used batch size described above, as I thought it is more natural settings. (increase batch sizes along with model size increases.) So batch size increasement can give additional stabilities for larger models. But, apparently, it is not enough for larger models as shown in vanilla settings.</p><p>I found Attention logit softcapping is not enough for reduce learning rate sensitivity, and is has similar curve to vanilla case. But, with post normalization (which is also used in Gemma 2), it is possible to have similar reduced sensitivity to QK norm. Again, post norm itself is not enough for stable training, as shown in 691M curves with post norm without softcapping.</p><p>Also, if comparing basin of learning rate sensitivity between 34M and 691M models, with Softcapping + Post norm or QK norm allows to use similar learning rate for both models. Apparently it is not possible with vanilla transformer. Learning rate left-shifts as model size increases.</p><p>And I constantly got a slightly better results with QK norm or Softcapping/Post Norm, about loss 0.01. Anyway it seems like that there is reason Gemma 2 (and Grok-1 <a href=https://github.com/xai-org/grok-1>https://github.com/xai-org/grok-1</a>) have used softcapping and post norm, as it is anyway possible to train models with that sizes in vanilla settings. Recently OLMoE also reported with QK norm they can achieve better results. I think it would be similar with softcapping and post norm.</p><h2 id=why-combination-of-softcapping-and-post-norm-helpful>Why combination of Softcapping and Post Norm helpful?</h2><p>It is known that QK norm suppresses unbounded increase of attention logits, thus stabilizes training <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>. But as shown in softcapping only results, bounding logit value itself not sufficient to stabilize training. Maybe it is also required to modulate distribution of attention logits.</p><p>Again, post normalization is somewhat helpful for stabilize training, but it is likely that absolute clipping of attention logit is required to further stabilization. So I suspect post norm is helpful for more sane overall distribution of logits, and softcapping is helpful to bound and remove outliers of logits. (CogView <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> and Swin Transformer V2 <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> reports using post normalization reduces maximum value or variance of features. Swin Transformer V2 also have used cosine similarity for attention logits, and it is reported that is makes &ldquo;milder&rdquo; attention logits.)</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726728655/scaling-law/cogview.png alt="Post Normalization &ldquo;Sandwich Norm&rdquo; in CogView. Sandwich Norm in CogView effectively suppresses maximum value of output embeddings."></p><blockquote><p>Pre Normalization and Post Normalization, or &ldquo;Sandwich Norm&rdquo; in CogView <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>. Sandwich Norm and subtraction of maximum value to prevent overflow effectively suppresses maximum value of output embeddings.</p></blockquote><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726728655/scaling-law/swin-transformer-v2.png alt="Signal propagation plot (Average feature variance) of Swin Transformer V2. Post normalization suppresses increase of average feature variance, compared to pre normalization."></p><blockquote><p>Signal propagation plot (Average feature variance) of Swin Transformer V2. Post normalization suppresses increase of average feature variance, compared to pre normalization across all model sizes and supervised or self-supervised training.</p></blockquote><p>It is well known fact that LLMs have outlier values in features <sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup>. Even though pre normalization normalizes input features, these outliers can cause distribution shift <sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup> <sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup>. Post normalization can alleviate this as shown in. But it is not enough for overall training stabilization, as query and key projection can still induces outliers or large attention logits. Softcapping is helpful for this by directly suppress maximum logits.</p><p>If your tasks is more picky like autoregressive text to image generation, you may also need to bound attention logits using QK norm and also post normalization as in <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. It might be due to gating of SwiGLU in FFN, as suggested in <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>.</p><p>This is mostly a kind of guesswork, surely more investigation is needed.</p><h2 id=do-we-need-to-increase-batch-size-along-with-model-size>Do we need to increase batch size along with model size?</h2><p>It is general knowledge that any batch size below &ldquo;critical batch size&rdquo; is okay for training of models <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>. But there are puzzling papers that reported there are also minimum batch sizes for training models that if smaller than that training loss will be increased <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</p><p>One thing make it hard to experiment with optimal batch sizes is that you need to adjusts many hyperparameters along with learning rates, especially Adam $\beta_1$ and $\beta_2$ <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>. It will be best to grid search with this, but given compute budgets, I have manually choosed candidate hyperparameters with learning rates and $\beta_2$. (Based on simple logic - smaller $\beta_2$ will be better for small batch sizes, and larger learning rates will be better for larger batch sizes.)</p><p>Maybe it will be also needed to adjust $\beta_1$ to get optimal results. But practically it makes problem harder for actual training, as it means that you need to adjust one more hyperparameters to get a optimal result for small batch sizes. So I think adjusting learning rate and $\beta_2$ is actually more practical setting. (More practically, trying to find minimum batch sizes that works well is not important. What we want to know is largests batch sizes that works well.)</p><p>In this experiment I have trained all of the models with same training tokens 15.7B (which corresponds to 30K steps with 512 batch size.) and number of training steps will be adjusted with batch size. I have decayed learning rate to 1e-5 of peak learning rate as this affects results a lot.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726710105/scaling-law/batch-sizes.png alt="Evaluation loss of models with different batch sizes"></p><p>And here is the results. I annotated as a optimal if loss is within 0.25% difference to optimal/minimum loss, which is similar to <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. (I think this is very important, as in many cases basins around optimal learning rate or batch size is quite flat. So if you just pick absolute minimum it could be misleading.)</p><p>So I found that maximum allowable batch sizes is increasing with model size. Also, I found that there are minimum batch sizes for each model that below of it there are training difficulties. I have found slight right-shift of optimal batch sizes with model sizes. It is hard to directly compare, but I think I found less severe than previous papers. I think candidate resons for this is 1. I have experimented only larger end of model sizes. 2. Right-shifting of batch siszes is less prominent than increase of allowable batch sizes 3. I have used fixed token budge for each model sizes, but previous papers have used chinchilla optimal scaling which increase token budgets along with model sizes. As it is common to use far more training tokens than chinchilla optimal regardless of model sizes, I think my setting could be useful for that scenario.</p><p>So why phenomena like this happens? I don&rsquo;t have theoretical explanation for this. But my guess is like this:</p><ol><li>Larger models are instable, and they benefit more from reduced noise of larger batch sizes.</li><li>Maybe larger models can better &ldquo;utilize&rdquo; informations from larger batch sizes.</li><li>Smaller models are harder to train, and they benefit from absolute increase of training steps. (smaller batch sizes.)</li></ol><p>As my guess was around training instabilities, I have thought that more instable models can have different patterns. As I have experimented with softcapping and post norm, I have tried to do similar experiments with vanilla transformer which could be more &ldquo;instable&rdquo;.</p><p>You can refer to this result from above plot with 238M Vanilla. It has reduced gaps between larger and smaller batch sizes by increased minimum loss, and has similar loss for small batch sizes. This is only partial evidence, but I think this suggests maybe there are reasons other than training stabilities for this batch size scaling phenonmena. I think more investigation will be meaningful for this problem.</p><h2 id=layer-stacking>Layer Stacking</h2><p>Training efficiencies, especially reducing number of training tokens given the loss or performances is now one of the most important things in LLM training. One part of problem is that as sheer amount of compute is needed, just slight reduction in training tokens could result in large amount of reduction of resources. (Or you can think this as a virtual increasing factor for resources, that if you can reduce 10% of training times, it could correspond to actual increasement of number of GPUs you have <a href=https://nonint.com/2023/11/05/compute-multipliers/>https://nonint.com/2023/11/05/compute-multipliers/</a>.) More importantly, as gathering data is bottleneck, reducing number of training tokens actually corresponds to increase of the size of the data you have. If you can reach same performance with 50% of training tokens, then it corresponds to 2x increase of total data.</p><p>There are promising approaches for this problem like RHO-Loss <sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup> <sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup> <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>. I also found stacking transformer layers for boosting training speed is interesting <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup> <sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup>. So I have tried to replicate results of <sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup>.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726711940/scaling-law/layer-stacking2.png alt="Training speed difference between layer stacking and from scratch for 7B model"></p><p>And the method is simple. You just need to stack transformer layers, like A, B, C to A, B, C, A, B, C. So I thought I can easily replicate their results, even there are some differences in training setups.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726711941/scaling-law/stacking.png alt="Failed run of replication of layer stacking that from scratch more gets better results at the end"></p><p>And I found that is is not an easy problem. There are many hyperparameters that governs early training dynamics like weight decay. And I found this hyperparameters can affect overall results by accelerating early trainings, but eventually at the final steps from scratch training gets better results than layer stacked models, which is worst thing can happen with training acceleration methods.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726711982/scaling-law/stacking3.png alt="More closely replicated experiment and better result for layer stacking"></p><p>So I have experimented with more similar settings to <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, especially sequence length, learning rate, and weight decay. I got slight acceleration for layer stacking, but it was not very large at this training stage, and it will be smaller if you also consider tokens used for training small models.</p><p>I think this worth investigating, but maybe I should be cautious more on exact training settings for replication.</p><h2 id=weight-decays>Weight Decays</h2><p>For above experiments I have used independent weight decay of 1e-4 <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. But after above experiments I wanted to investigate more on weight decay. Especially weight decay scheduling which appears in PaLM <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>.</p><p>Scheduling weight decay is not common, and I can&rsquo;t find many previous examples for this <sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup> <sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup>. But recently Llama 3 have reported that they have used weight decay scheduling for scaling experiments <sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup>.</p><p>Weight decay scheduling, which appears in PaLM, is setting weight decay to proportional to learning rates, line $weight\ decay = lr^2$. Llama 3 have used $weight\ decay = 0.1 imes lr$ for scaling experiments, which would be equivalent to $weight\ decay = 0.1 \times lr^2$ in PaLM <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> as PaLM uses Adafactor <sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup> which uses independent weight decay in default, in contrast to PyTorch or Optax implementation of AdamW that may have been used in Llama 3 which uses coupled weight decay to learning rates.</p><p>What would be meaning of this scheduling? First, if you are using coupled weight decay than using larger learning rate also means larger weight decay, so it can hinder training. But if you are using weight decay scheduling then weight decay will be small at the final stage of training, and it could be beneficial by avoiding using too large weight decay. Also it could be useful for model performances as it would reduce regularizing effect of weight decay at the final stage thus reduce bias of the model.</p><p>So I have experimented with various settings of weight decay. 1. PyTorch/Optax default weight decay which couples learning rate with weight decay. 2. Independent weight decay of 1e-4 which corresponds to 1e-4 / peak learning rate in PyTorch or Optax. 3. Weight decay scheduling of $weight\ decay = lr$. I have trained 724M model during 50K steps with various learning rates.</p><p><img src=https://res.cloudinary.com/rosinality/image/upload/v1726713898/scaling-law/weight-decay.png alt="Learning rate sensitivity of weight decay settings"></p><p>Similar with results in <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>, I found that using default coupled weight decay hinders training. Using independent weight decay relieves this and widens optimal learning rate basins. (But this is not useful for vanilla transformer as it will blow-up at large learning rates anyway.) Weight decay scheduling has similar effect to independent weight decay, and it is less effective at largest learning rates than independent weight decay, but it can reach slight better loss at optimal learning ratess, potentially due to reducing bias of the model.</p><p>It is ironic that independent weight decay which decouples weight decay from learning rates, and weight decay scheduling which couples weight decay with learning rate more has similar effect on training. Practically using weight decay scheduling could be dangerous as it tends to have less weight decay if you compare with coupled weight decay, and increasing scale of it by $weight\ decay = 10 \times lr$ was not work very well. I am very interested in weight decay of actuall Llama 3 405B training.</p><h2 id=conclusion>Conclusion</h2><p>Similar to previous experiments, <a href=https://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/>Preliminary Explorations on UL2 and Second-order Optimizers</a> I was able to get (at least for me) interesting results that is hard to get without large amount of computes. Thank you again for TPU Research Cloud, and I am not more falling in love with TPU.</p><p>Many things like scaling law estimation, learning rate sensitivity with architectural modifications, estimating optimal batch sizes, and investigating of layer stacking is almost direct replication efforts of previous papers. But I think softcapping and post normalization results and weight decay scheduling could be interesting to others.</p><hr><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., &mldr; & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. <a href=https://arxiv.org/abs/2001.08361>https://arxiv.org/abs/2001.08361</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., &mldr; & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. <a href=https://arxiv.org/abs/2203.15556>https://arxiv.org/abs/2203.15556</a> <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., &mldr; & Zou, Y. (2024). Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954. <a href=https://arxiv.org/abs/2401.02954>https://arxiv.org/abs/2401.02954</a> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Xie, X., Ding, K., Yan, S., Toh, K. C., & Wei, T. (2024). Optimization Hyper-parameter Laws for Large Language Models. arXiv preprint arXiv:2409.04777. <a href=https://arxiv.org/abs/2409.04777>https://arxiv.org/abs/2409.04777</a> <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Ge, C., Ma, Z., Chen, D., Li, Y., & Ding, B. (2024). Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining. arXiv preprint arXiv:2405.14908. <a href=https://arxiv.org/abs/2405.14908>https://arxiv.org/abs/2405.14908</a> <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>Porian, T., Wortsman, M., Jitsev, J., Schmidt, L., & Carmon, Y. (2024). Resolving Discrepancies in Compute-Optimal Scaling of Language Models. arXiv preprint arXiv:2406.19146. <a href=https://arxiv.org/abs/2406.19146>https://arxiv.org/abs/2406.19146</a> <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., &mldr; & Ronstrom, S. (2024). Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118. <a href=https://arxiv.org/abs/2408.00118>https://arxiv.org/abs/2408.00118</a> <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., &mldr; & Gao, J. (2022). Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466. <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>Wortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A., Adlam, B., &mldr; & Kornblith, S. (2023). Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322. <a href=https://arxiv.org/abs/2309.14322>https://arxiv.org/abs/2309.14322</a> <a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>Everett, K., Xiao, L., Wortsman, M., Alemi, A. A., Novak, R., Liu, P. J., &mldr; & Pennington, J. (2024). Scaling Exponents Across Parameterizations and Optimizers. arXiv preprint arXiv:2407.05872. <a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11 role=doc-endnote><p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., &mldr; & Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311. <a href=https://arxiv.org/abs/2204.02311>https://arxiv.org/abs/2204.02311</a> <a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12 role=doc-endnote><p>Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., &mldr; & Houlsby, N. (2023, July). Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning (pp. 7480-7512). PMLR. <a href=https://arxiv.org/abs/2305.18245>https://arxiv.org/abs/2305.18245</a> <a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13 role=doc-endnote><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &mldr; & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762. <a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a> <a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14 role=doc-endnote><p>Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., &mldr; & Liu, T. Y. (2020). On Layer Normalization in the Transformer Architecture. arXiv preprint arXiv:2002.04745. <a href=https://arxiv.org/abs/2002.04745>https://arxiv.org/abs/2002.04745</a> <a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15 role=doc-endnote><p>Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., & Wei, F. (2022). DeepNet: Scaling Transformers to 1,000 Layers. arXiv preprint arXiv:2203.00555. <a href=https://arxiv.org/abs/2203.00555>https://arxiv.org/abs/2203.00555</a> <a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16 role=doc-endnote><p>Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., &mldr; & Guo, B. (2021). Swin Transformer V2: Scaling Up Capacity and Resolution. arXiv preprint arXiv:2111.09883. <a href=https://arxiv.org/abs/2111.09883>https://arxiv.org/abs/2111.09883</a> <a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17 role=doc-endnote><p>Chameleon Team. (2024). Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818. <a href=https://arxiv.org/abs/2405.09818>https://arxiv.org/abs/2405.09818</a> <a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18 role=doc-endnote><p>Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., &mldr; & Tang, J. (2021). CogView: Mastering Text-to-Image Generation via Transformers. arXiv preprint arXiv:2105.13290. <a href=https://arxiv.org/abs/2105.13290>https://arxiv.org/abs/2105.13290</a> <a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19 role=doc-endnote><p>Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). LLM. int8 (): 8-bit Matrix Multiplication for Transformers at Scale. arXiv preprint arXiv:2208.07339. <a href=https://arxiv.org/abs/2208.07339>https://arxiv.org/abs/2208.07339</a> <a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20 role=doc-endnote><p>Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., & Han, S. (2022). SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. arXiv preprint arXiv:2211.10438. <a href=https://arxiv.org/abs/2211.10438>https://arxiv.org/abs/2211.10438</a> <a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21 role=doc-endnote><p>Bondarenko, Y., Nagel, M., & Blankevoort, T. (2023). Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. arXiv preprint arXiv:2306.12929. <a href=https://arxiv.org/abs/2306.12929>https://arxiv.org/abs/2306.12929</a> <a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22 role=doc-endnote><p>Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8110-8119). <a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23 role=doc-endnote><p>Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., & Laine, S. (2023). Analyzing and Improving the Training Dynamics of Diffusion Models. arXiv preprint arXiv:2312.02696. <a href=https://arxiv.org/abs/2312.02696>https://arxiv.org/abs/2312.02696</a> <a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24 role=doc-endnote><p>McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162. <a href=https://arxiv.org/abs/1812.06162>https://arxiv.org/abs/1812.06162</a> <a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25 role=doc-endnote><p>Hilton, J., Cobbe, K., & Schulman, J. (2022). Batch size-invariance for policy optimization. Advances in Neural Information Processing Systems, 35, 17086-17098. <a href=https://arxiv.org/abs/2206.05131>https://arxiv.org/abs/2206.05131</a> <a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26 role=doc-endnote><p>Mindermann, S., Brauner, J., Razzak, M., Sharma, M., Kirsch, A., Xu, W., &mldr; & Gal, Y. (2022). Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. arXiv preprint arXiv:2206.07137. <a href=https://arxiv.org/abs/2206.07137>https://arxiv.org/abs/2206.07137</a> <a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27 role=doc-endnote><p>Evans, T., Parthasarathy, N., Merzic, H., & Henaff, O. J. (2024). Data curation via joint example selection further accelerates multimodal learning. arXiv preprint arXiv:2406.17711. <a href=https://arxiv.org/abs/2406.17711>https://arxiv.org/abs/2406.17711</a> <a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28 role=doc-endnote><p>Du, W., Luo, T., Qiu, Z., Huang, Z., Shen, Y., Cheng, R., &mldr; & Fu, J. (2024). Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training. arXiv preprint arXiv:2405.15319. <a href=https://arxiv.org/abs/2405.15319>https://arxiv.org/abs/2405.15319</a> <a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29 role=doc-endnote><p>Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., &mldr; & Irving, G. (2021). Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446. <a href=https://arxiv.org/abs/2112.11446>https://arxiv.org/abs/2112.11446</a> <a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30 role=doc-endnote><p>Lewkowycz, A., & Gur-Ari, G. (2020). On the training dynamics of deep networks with $ L_2 $ regularization. arXiv preprint arXiv:2006.08643. <a href=https://arxiv.org/abs/2006.08643>https://arxiv.org/abs/2006.08643</a> <a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31 role=doc-endnote><p>Golatkar, A., Achille, A., & Soatto, S. (2019). Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence. arXiv preprint arXiv:1905.13277. <a href=https://arxiv.org/abs/1905.13277>https://arxiv.org/abs/1905.13277</a> <a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32 role=doc-endnote><p>Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., &mldr; & Ganapathy, R. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. <a href=https://arxiv.org/abs/2407.21783>https://arxiv.org/abs/2407.21783</a> <a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33 role=doc-endnote><p>Shazeer, N., & Stern, M. (2018). Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. arXiv preprint arXiv:1804.04235. <a href=https://arxiv.org/abs/1804.04235>https://arxiv.org/abs/1804.04235</a> <a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small"></span><br><a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/deep-learning/>deep learning</a>
<a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/optimization/>optimization</a>
<a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/architecture/>architecture</a>
<a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/scaling-law/>scaling law</a>
<a class="tag tag--primary tag--small" href=http://rosinality.github.io//tags/language-models/>language models</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=/2025/05/%ED%81%B4%EB%A0%88%EB%A5%B4-%EC%98%B5%EC%8A%A4%ED%80%B4%EB%A5%B4-33-%EC%9B%90%EC%A0%95%EB%8C%80/ data-tooltip="클레르 옵스퀴르: 33 원정대"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml"></span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/ data-tooltip="Preliminary Explorations on UL2 and Second-order Optimizers"><span class="hide-xs hide-sm text-small icon-mr"></span><i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=http%3a%2f%2frosinality.github.io%2f2024%2f09%2fscaling-law-architecture-for-stability-and-layer-stacking%2f"><i class="fa fa-google-plus"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frosinality.github.io%2f2024%2f09%2fscaling-law-architecture-for-stability-and-layer-stacking%2f"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=http%3a%2f%2frosinality.github.io%2f2024%2f09%2fscaling-law-architecture-for-stability-and-layer-stacking%2f"><i class="fa fa-twitter"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><span class=copyrights>&copy; 2025 Kim Seonghyeon.</span></footer></div><div id=share-options-bar class=share-options-bar data-behavior=5><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=http%3a%2f%2frosinality.github.io%2f2024%2f09%2fscaling-law-architecture-for-stability-and-layer-stacking%2f"><i class="fa fa-google-plus"></i><span></span></a></li><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2frosinality.github.io%2f2024%2f09%2fscaling-law-architecture-for-stability-and-layer-stacking%2f"><i class="fa fa-facebook-official"></i><span></span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=http%3a%2f%2frosinality.github.io%2f2024%2f09%2fscaling-law-architecture-for-stability-and-layer-stacking%2f"><i class="fa fa-twitter"></i><span></span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt><h4 id=about-card-name>Kim Seonghyeon</h4><div id=about-card-bio>Machine learning enthusiast</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Graduate student in HCCLab at Seoul National University</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Korea, Republic of</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input type=text id=algolia-search-input name=search class="form-control input--large search-input" placeholder></form></div><div class=modal-body><div class="no-result text-color-light text-center"></div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/post/><h3 class=media-heading>Posts</h3></a><span class=media-meta><span class="media-date text-small">May 5, 2025</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2025/05/%ED%81%B4%EB%A0%88%EB%A5%B4-%EC%98%B5%EC%8A%A4%ED%80%B4%EB%A5%B4-33-%EC%9B%90%EC%A0%95%EB%8C%80/><h3 class=media-heading>클레르 옵스퀴르: 33 원정대</h3></a><span class=media-meta><span class="media-date text-small">May 5, 2025</span></span><div class="media-content hide-xs font-merryweather">오랜만에 좋은 게임을 했다. 새삼스럽지만 게임은 굉장히 강력한 스토리텔링의 수단이라는 생각을 한다. 게임 플레이와 섞이기 때문에 서사의 밀도가 높지는 않겠지만 2</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/><h3 class=media-heading>Scaling Law, Architecture for Stability and Layer Stacking</h3></a><span class=media-meta><span class="media-date text-small">Sep 9, 2024</span></span><div class="media-content hide-xs font-merryweather">Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/><h3 class=media-heading>Preliminary Explorations on UL2 and Second-order Optimizers</h3></a><span class=media-meta><span class="media-date text-small">Jun 6, 2024</span></span><div class="media-content hide-xs font-merryweather">In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/07/constitutional-ai/><h3 class=media-heading>Constitutional AI</h3></a><span class=media-meta><span class="media-date text-small">Jul 7, 2023</span></span><div class="media-content hide-xs font-merryweather">Helpful & Harmless Agent AI 모델의 정렬(Alignment)이라고 이야기할 때 흔히 나오는 Helpfulness와 Harmlessness는 어떤 의미인가? 이는 정의</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%99%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4/><h3 class=media-heading>이미지와 텍스트 생성 모델에 대해</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">이미지 생성 하면 Style GAN이었던 시절에도 일러스트 생성 등은 오타쿠적 인기가 있는 주제였다. 문제의 Danbooru 데이터셋 같은 경우에도 그 시점에 이미 만들어진 데이터셋이었</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%96%B8%EC%96%B4%EC%9D%98-%EC%86%90%EC%8B%A4-%EC%95%95%EC%B6%95%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/><h3 class=media-heading>언어의 손실 압축에 대하여</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web LM을 다음 단어를 예측할 뿐이라거나 학습 데이터를 기억할 뿐이라는 식으로 묘사하는 것은 폄하를 위한 언어이지 LM의 실체나 실제 한계에 대해서 논하기에 적절한 방</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/01/ocr-%ED%9A%8C%EA%B3%A0/><h3 class=media-heading>OCR 회고</h3></a><span class=media-meta><span class="media-date text-small">Jan 1, 2023</span></span><div class="media-content hide-xs font-merryweather">타이틀 커버 이미지 출처: https://www.behance.net/gallery/6146939/OCR-A-Poster/modules/152114859 4년 동안 몰두했던 OCR이라는 주제를 마무리하게 되면서 으레 그래왔듯 회고를 남겨본다. 이랬더라면 어땠을까 같은 소소한 소회보다는</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-7/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 7</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-6/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 6</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero data-message-one data-message-other>69 posts found</p></div></div></div><div id=cover style=background-image:url(http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/stair.jpg)></div><script>(function(d){var config={kitId:'vld6lxf',scriptTimeout:3000,async:true},h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)})(document);</script><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js></script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    preview: "none",
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script><script src=/js/script-wl33z0n6ocaypepiqrazthtivfrliqijej4rq8ek8gvrv1awftmgjuv8k4zc.min.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight').each(function(i,block){var code="";hljs.highlightAuto(block.innerText).value.split(/\r\n|\r|\n/).forEach(function(line){code+="<span class=\"line\">"+line+"</span><br>";});if(code.length>0){block.innerHTML=code;}});$('pre > code').each(function(i,block){$(this).addClass('codeblock');hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='http:\/\/rosinality.github.io\/2024\/09\/scaling-law-architecture-for-stability-and-layer-stacking\/';this.page.identifier='\/2024\/09\/scaling-law-architecture-for-stability-and-layer-stacking\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='rosinality';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script></body></html>