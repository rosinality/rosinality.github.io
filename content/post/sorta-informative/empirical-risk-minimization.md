+++
autoThumbnailImage = true
categories = ["sorta informative"]
coverImage = "http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/exit.jpg"
coverSize = "partial"
date = "2017-04-21T16:02:37+09:00"
hasMath = false
isCJKLanguage = true
keywords = ["empirical risk minimization", "machine learning"]
tags = ["empirical risk minimization", "machine learning"]
thumbnailImagePosition = "top"
title = "경험적 위험 최소화"

+++

Breiman이 Statistical Modeling: The Two Cultures에서 보여줬던 것처럼 통계적 모델링에는 대체로 두 가지 경로가 있다. 하나는 데이터의 생성 과정을 기술하여 데이터를 모델링하고 동시에 모델의 설계자가 관심을 가지고 있었던 모형 내의 확률 변수와 파라미터를 분석하는 것에 집중하는 접근. 다른 하나는 예측 성능 등을 최대화하는 것에 방점을 데이터를 모델링하는 접근.

예측 성능을 최대화하는 것에 집중하는 접근은 세련된 데이터 생성 과정에 대한 모형과 비교하면 좀 무식한 함수 최적화에 가깝게 보이는 경향이 있다. 예를 들어 Kaggle맨들의 영원한 사랑 GBDT 같은 경우엔 loss의 n-1번째 트리까지의 합에 대한 그래디언트, 일종의 잔차에 피팅한 n번째 트리를 더하는 방식으로 트레이닝한다. 간단하게 표현하면 회귀 모형을 피팅한 다음에 남은 잔차에 다시 회귀 모형을 피팅해서 이전 모형에 합치는 것이다. 데이터 생성 모형에 비하면 우아한 맛이 떨어진다면 떨어질 수 있겠다.

이런 무식한(?) 함수 최적화를 정당화하는 논리가 Empirical Risk Minimization이다. 목표는 정확도를 높이는 것이다. 그런데 정확도는 최적화하기 어렵거나 불가능한 목표다. 그러니까 여기서도 정확도 혹은 그에 준하는 loss를 직접 사용하는 대신 최적화가 용이한 loss 함수를 사용한다. loss는 학습할 함수 f(x)와 예측하려는 결과 y 사이의 차이에 대한 함수다. 따라서 데이터 생성 분포 p(x, y)에 대해 loss(f(x), y)의 기대값을 최소화하는 것으로 정확도를 높일 수 있다는 것이다.

물론 p(x, y)에 대해 기대값을 구한다는 것은 불가능하다. 그러나 p(x, y)에 의해 샘플링된 데이터들을 통해 경험적 분포empirical distribution에 대해 기대값을 구한다는 것은 가능하다. 그러니까 결국 데이터에 대해 loss를 최소화하도록 함수를 최적화하면, 데이터에 잘 맞는 함수를 구하면 된다는 것으로 돌아온다.

그렇지만 단순히 이런 함수를 구한다는 것만으로는 일반화를 할 수 있다는 보장이 없다. 데이터에 대해 loss를 최소화하는 가장 쉬운 방법은 데이터를 함수에 모두 저장해놓는 것이다. 물론 전혀 일반화를 할 수 없는 방법이다. 따라서 loss를 최소화하되 또 적당히(?) 일반화할 수 있는 방식으로 최적화를 해야 하는 것이다.

데이터 생성 모형과 예측 성능에 집중하는 모형을 딱 구분하기는 어렵지만, 일단 소위 블랙박스 모형으로 한정한다고 하면 대체로 예측 성능에 집중하는 모형이 예측 성능에는 일반적으로 더 뛰어나다(물론 당연한 소리다). 이걸 어떻게 봐야할 것인가는 여전히 다들 생각이 다른 듯 하다. 해석의 용이성을 높게 치는 쪽이 있고 예측의 정확성을 더 높게 치는 쪽이 있고. 나는 일단 후자가 더 나은, 또는 더 안전한 접근이라고 본다. 예컨대 y에 대해 x와 z1, z2, ... 사이에 상호작용이 있다고 하자. 그 상호작용의 양상에 대해서 모르는 상황에선 데이터 생성 모형이나 예측 모형이나 상호작용에 대해서 파악할 수 없다는 것은 마찬가지다. 그러나 데이터 생성 모형에서 상호작용이 빠지면 y에 대한 x의 관계는 다른 변수와의 상호작용이 배제된 상태에서의 관계가 나타나게 된다. 그러나 상호작용을 자동으로 발견하는 예측 모형에서라면, 상호작용에 대해서 파악하지 못하고 있는 상태에서도 y에 대한 x의 관계는 상호작용이 포함된 상태에서의 관계가 나타나게 된다. 상호작용이 많은 경우에 y에 대한 x의 관계를 크게 변화시킨다는 것을 고려할 때 이런 특성은 y와 x의 관계를 오인하지 않는데 도움이 될 수도 있다.