<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Nondifferentiable Log</title><link>http://rosinality.github.io/post/</link><description>Recent content in Posts on Nondifferentiable Log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Tue, 04 Jun 2024 07:28:30 +0900</lastBuildDate><atom:link href="http://rosinality.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Preliminary Explorations on UL2 and Second-order Optimizers</title><link>http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/</link><pubDate>Tue, 04 Jun 2024 07:28:30 +0900</pubDate><guid>http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/</guid><description>In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</description></item><item><title>오펜하이머</title><link>http://rosinality.github.io/2023/11/%EC%98%A4%ED%8E%9C%ED%95%98%EC%9D%B4%EB%A8%B8/</link><pubDate>Sat, 04 Nov 2023 22:25:40 +0900</pubDate><guid>http://rosinality.github.io/2023/11/%EC%98%A4%ED%8E%9C%ED%95%98%EC%9D%B4%EB%A8%B8/</guid><description>영화의 가장 중요한 질문. 청문회에서 오펜하이머가 본 환영. 왜 지금 후회하고 반대할 일들을 그 당시에는 예측하지 못했는가? 아니 사실 알고 있지 않았는가? 그런데 알</description></item><item><title>그대들은 어떻게 살 것인가</title><link>http://rosinality.github.io/2023/11/%EA%B7%B8%EB%8C%80%EB%93%A4%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%B4-%EA%B2%83%EC%9D%B8%EA%B0%80/</link><pubDate>Sat, 04 Nov 2023 21:23:58 +0900</pubDate><guid>http://rosinality.github.io/2023/11/%EA%B7%B8%EB%8C%80%EB%93%A4%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%B4-%EA%B2%83%EC%9D%B8%EA%B0%80/</guid><description>이것은 상징이라고 이렇게까지 말하는 영화는 그리 많지 않을 듯 싶다. 상징이라는 것은 분명한데 이것이 무엇의 상징인지, 그리고 그 상징들이 모여서 어떤 한 가지를 말하</description></item><item><title>트랜스포머 모델 선택의 기원에 대하여</title><link>http://rosinality.github.io/2023/11/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D%EC%9D%98-%EA%B8%B0%EC%9B%90%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</link><pubDate>Fri, 03 Nov 2023 18:27:16 +0900</pubDate><guid>http://rosinality.github.io/2023/11/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EB%AA%A8%EB%8D%B8-%EC%84%A0%ED%83%9D%EC%9D%98-%EA%B8%B0%EC%9B%90%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</guid><description>트랜스포머는 이제 너무나 유명한 모델이고 아마 딥 러닝 모델 중에서 트랜스포머보다 많은 설명이 작성된 모델 또한 없을 것 같다. 그런 상황에서 트랜스포머의 디자인에 대</description></item><item><title>Constitutional AI</title><link>http://rosinality.github.io/2023/07/constitutional-ai/</link><pubDate>Mon, 31 Jul 2023 11:55:37 +0900</pubDate><guid>http://rosinality.github.io/2023/07/constitutional-ai/</guid><description>Helpful &amp;amp; Harmless Agent AI 모델의 정렬(Alignment)이라고 이야기할 때 흔히 나오는 Helpfulness와 Harmlessness는 어떤 의미인가? 이는 정의</description></item><item><title>이미지와 텍스트 생성 모델에 대해</title><link>http://rosinality.github.io/2023/02/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%99%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4/</link><pubDate>Wed, 15 Feb 2023 23:30:19 +0900</pubDate><guid>http://rosinality.github.io/2023/02/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%99%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4/</guid><description>이미지 생성 하면 Style GAN이었던 시절에도 일러스트 생성 등은 오타쿠적 인기가 있는 주제였다. 문제의 Danbooru 데이터셋 같은 경우에도 그 시점에 이미 만들어진 데이터셋이었</description></item><item><title>언어의 손실 압축에 대하여</title><link>http://rosinality.github.io/2023/02/%EC%96%B8%EC%96%B4%EC%9D%98-%EC%86%90%EC%8B%A4-%EC%95%95%EC%B6%95%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</link><pubDate>Wed, 15 Feb 2023 23:27:49 +0900</pubDate><guid>http://rosinality.github.io/2023/02/%EC%96%B8%EC%96%B4%EC%9D%98-%EC%86%90%EC%8B%A4-%EC%95%95%EC%B6%95%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/</guid><description>https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web LM을 다음 단어를 예측할 뿐이라거나 학습 데이터를 기억할 뿐이라는 식으로 묘사하는 것은 폄하를 위한 언어이지 LM의 실체나 실제 한계에 대해서 논하기에 적절한 방</description></item><item><title>OCR 회고</title><link>http://rosinality.github.io/2023/01/ocr-%ED%9A%8C%EA%B3%A0/</link><pubDate>Mon, 02 Jan 2023 22:49:14 +0900</pubDate><guid>http://rosinality.github.io/2023/01/ocr-%ED%9A%8C%EA%B3%A0/</guid><description>타이틀 커버 이미지 출처: https://www.behance.net/gallery/6146939/OCR-A-Poster/modules/152114859 4년 동안 몰두했던 OCR이라는 주제를 마무리하게 되면서 으레 그래왔듯 회고를 남겨본다. 이랬더라면 어땠을까 같은 소소한 소회보다는</description></item><item><title>텔 아비브와 ECCV 2022 여행기 7</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-7/</link><pubDate>Mon, 07 Nov 2022 16:16:14 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-7/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>텔 아비브와 ECCV 2022 여행기 6</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-6/</link><pubDate>Mon, 07 Nov 2022 16:16:12 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-6/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>텔 아비브와 ECCV 2022 여행기 5</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-5/</link><pubDate>Mon, 07 Nov 2022 16:16:11 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-5/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>텔 아비브와 ECCV 2022 여행기 4</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-4/</link><pubDate>Mon, 07 Nov 2022 12:31:27 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-4/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>텔 아비브와 ECCV 2022 여행기 3</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-3/</link><pubDate>Mon, 07 Nov 2022 11:49:04 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-3/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>텔 아비브와 ECCV 2022 여행기 2</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-2/</link><pubDate>Fri, 04 Nov 2022 17:51:06 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-2/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>텔 아비브와 ECCV 2022 여행기 1</title><link>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-1/</link><pubDate>Thu, 03 Nov 2022 11:08:16 +0900</pubDate><guid>http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-1/</guid><description>텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</description></item><item><title>머신 러닝 시스템에서 설정 관리하기</title><link>http://rosinality.github.io/2021/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%97%90%EC%84%9C-%EC%84%A4%EC%A0%95-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/</link><pubDate>Sat, 22 May 2021 20:27:53 +0900</pubDate><guid>http://rosinality.github.io/2021/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%97%90%EC%84%9C-%EC%84%A4%EC%A0%95-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/</guid><description>머신 러닝 코드, 특히 실험적 목적이 강한 코드에서 가장 중요한 문제 중 하나가 설정을 관리하는 것이라고 본다. 머신 러닝 모델에는 수많은 하이퍼파라미터가 존재하고 그</description></item><item><title>진격의 거인</title><link>http://rosinality.github.io/2021/04/%EC%A7%84%EA%B2%A9%EC%9D%98-%EA%B1%B0%EC%9D%B8/</link><pubDate>Sat, 10 Apr 2021 14:51:22 +0900</pubDate><guid>http://rosinality.github.io/2021/04/%EC%A7%84%EA%B2%A9%EC%9D%98-%EA%B1%B0%EC%9D%B8/</guid><description>진격의 거인은 우익적 혹은 극우적인가? 보통 일본의 창작물이 극우적인가를 논하는 기준은 욱일기가 은근슬쩍 등장한다든지 혹은 대체역사물 같이 노골적으로 (유치한</description></item><item><title>The Last of Us Part II</title><link>http://rosinality.github.io/2020/06/the-last-of-us-part-ii/</link><pubDate>Sat, 27 Jun 2020 14:35:00 +0900</pubDate><guid>http://rosinality.github.io/2020/06/the-last-of-us-part-ii/</guid><description>과연 라스트 오브 어스 2는 증오와 적대에 대한 게임이다. (인터뷰) 게임에서 싸우는 것이 일상이고 증오와 적대가 게임에 등장하는 것이야 대수로운 일도 아니지 않을까</description></item><item><title>머신러닝 파이프라인 만들기</title><link>http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/</link><pubDate>Wed, 14 Aug 2019 13:26:39 +0900</pubDate><guid>http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/</guid><description>딥 러닝이 유행하기 시작할 무렵 딥 러닝의 장점으로 나왔던 것이 특징을 추출하는 알고리즘(Feature extractor)을 데이터를 통해 학습한다는 것이었</description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</link><pubDate>Tue, 16 Oct 2018 10:03:59 +0900</pubDate><guid>http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</guid><description>프리트레이닝과 전이학습 모델을 프리트레이닝하는 것이, 혹은 프리트레이닝된 모델이 모듈로 쓰는 것이 성능에 큰 영향을 미칠 수 있다는 건 너무나 잘 알려진 사실이다.</description></item><item><title>인과관계를 어떻게 밝힐 수 있는가</title><link>http://rosinality.github.io/2018/08/%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%B0%9D%ED%9E%90-%EC%88%98-%EC%9E%88%EB%8A%94%EA%B0%80/</link><pubDate>Sun, 26 Aug 2018 22:41:37 +0900</pubDate><guid>http://rosinality.github.io/2018/08/%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%B0%9D%ED%9E%90-%EC%88%98-%EC%9E%88%EB%8A%94%EA%B0%80/</guid><description>우선 인과관계라는 것이 대체 무엇인가? 인과관계라는 것을 명확하게 정의하는 것은 복잡한 철학적 문제다. 그러나 실용적인 목적으로는 단순한 정의를 사용할 수 있다.</description></item><item><title>버닝</title><link>http://rosinality.github.io/2018/05/%EB%B2%84%EB%8B%9D/</link><pubDate>Mon, 21 May 2018 10:03:59 +0900</pubDate><guid>http://rosinality.github.io/2018/05/%EB%B2%84%EB%8B%9D/</guid><description>모호한 세상. 영화에서는 수수께끼 같다는 말로 직접적으로 표현한다. 세상을 명료하게 설명해서 글로 옮길 수 있다면 좋겠지만 영화는 시종일관 진실이 무엇인지 말해주</description></item><item><title>앞으로 재미있을지도 모르는 주제들</title><link>http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/</link><pubDate>Fri, 03 Nov 2017 10:03:59 +0900</pubDate><guid>http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/</guid><description>LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거</description></item><item><title>옥자</title><link>http://rosinality.github.io/2017/06/%EC%98%A5%EC%9E%90/</link><pubDate>Thu, 29 Jun 2017 19:31:03 +0900</pubDate><guid>http://rosinality.github.io/2017/06/%EC%98%A5%EC%9E%90/</guid><description>스포일러 다수 포함 2시간을 금방 보내기는 했는데 보고 나서 바로 든 생각은 어 이걸로 끝?에 가까웠다. 스토리는 굉장히 단순하고 예상 가능한 방식으로 움직인다. 너무</description></item><item><title>강화학습에 대해</title><link>http://rosinality.github.io/2017/05/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%97%90-%EB%8C%80%ED%95%B4/</link><pubDate>Sun, 21 May 2017 00:01:52 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%97%90-%EB%8C%80%ED%95%B4/</guid><description>지금의 강화학습은 행동주의 심리학의 조작적 조건 형성과 많이 닮아있다. 사실 강화Reinforcement라는 단어를 쓰는 것 자체가 행동주의적 심리학의 언어</description></item><item><title>자유주의적 정치의 이상</title><link>http://rosinality.github.io/2017/05/%EC%9E%90%EC%9C%A0%EC%A3%BC%EC%9D%98%EC%A0%81-%EC%A0%95%EC%B9%98%EC%9D%98-%EC%9D%B4%EC%83%81/</link><pubDate>Mon, 08 May 2017 22:50:05 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%EC%9E%90%EC%9C%A0%EC%A3%BC%EC%9D%98%EC%A0%81-%EC%A0%95%EC%B9%98%EC%9D%98-%EC%9D%B4%EC%83%81/</guid><description>어떤 이들에게 정치의 가장 이상적인 형태는 의회에서 일어나고 있는 형태의 정치다. 끊임없이 토론하고, 상대를 존중하고, 협상과 양보를 통해서 서로 원하는 바를 (절</description></item><item><title>머신 러닝과 사회 과학</title><link>http://rosinality.github.io/2017/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C-%EA%B3%BC%ED%95%99/</link><pubDate>Mon, 08 May 2017 22:45:30 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C-%EA%B3%BC%ED%95%99/</guid><description>머신 러닝이 사회과학 연구에 어떤 도움이 될까? 머신 러닝 알고리즘 중에는 사회과학에서 흔히 쓰이던 방법과 거의 동일한 작업을 수행할 수 있으면서 많은 경우에 더 강력한</description></item><item><title>특징 추출(Feature Extraction)과 딥 러닝</title><link>http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/</link><pubDate>Mon, 08 May 2017 22:41:37 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/</guid><description>https://sinews.siam.org/Details-Page/deep-deep-trouble 뉴럴넷 연구를 하던 사람들이 오랜 겨울을 지나왔던 것처럼 이미지 처리에서, 이젠 전통적인 방법이라고 불리는 방법들을 연구하던 사람들의 고민이 깊은 모양이다. 뉴</description></item><item><title>머신 러닝 모델을 만들기</title><link>http://rosinality.github.io/2017/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EB%A7%8C%EB%93%A4%EA%B8%B0/</link><pubDate>Mon, 08 May 2017 22:31:57 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EB%A7%8C%EB%93%A4%EA%B8%B0/</guid><description>머신 러닝을 하는데 성능이 잘 안 나온다는 질문이 종종 보여서, 모델 성능 평가의 왕도를 소개해봄. 사실 Deep Learning book의 챕터 11에 나오는 내용. 전처리를 하고 데이터</description></item><item><title>더불어민주당</title><link>http://rosinality.github.io/2017/05/%EB%8D%94%EB%B6%88%EC%96%B4%EB%AF%BC%EC%A3%BC%EB%8B%B9/</link><pubDate>Mon, 08 May 2017 22:29:19 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%EB%8D%94%EB%B6%88%EC%96%B4%EB%AF%BC%EC%A3%BC%EB%8B%B9/</guid><description>지금은 민주당이 아니면 쳐다보지도 않는 민주당 지지자지만 본래 민주당을 그렇게 지지하지는 않았었음. 소위 진보 정당들을 지지까지는 아니더라도 지금보다 훨씬 긍정</description></item><item><title>2017 대선 여론조사 분석</title><link>http://rosinality.github.io/2017/05/2017-%EB%8C%80%EC%84%A0-%EC%97%AC%EB%A1%A0%EC%A1%B0%EC%82%AC-%EB%B6%84%EC%84%9D/</link><pubDate>Mon, 01 May 2017 03:05:27 +0900</pubDate><guid>http://rosinality.github.io/2017/05/2017-%EB%8C%80%EC%84%A0-%EC%97%AC%EB%A1%A0%EC%A1%B0%EC%82%AC-%EB%B6%84%EC%84%9D/</guid><description>우선, 2012년에는 우선 2012년 여론조사와 실제 대선 결과를 비교해보면 다음과 같다. 여론조사 결과의 경우 합이 100이 되도록 각 값을 값의 합으로 나눴다.</description></item><item><title>Caffe2에 대해</title><link>http://rosinality.github.io/2017/04/caffe2%EC%97%90-%EB%8C%80%ED%95%B4/</link><pubDate>Tue, 25 Apr 2017 15:28:52 +0900</pubDate><guid>http://rosinality.github.io/2017/04/caffe2%EC%97%90-%EB%8C%80%ED%95%B4/</guid><description>PyTorch랑 장기적으로 백엔드를 통합한다니까 관심이 생겨서 Caffe2를 좀 들여다봤음. Caffe를 써본 적은 없음. protocol buffer 파일을 만들어서 돌린다는 것</description></item><item><title>Caffe2와 딥 러닝 프레임워크</title><link>http://rosinality.github.io/2017/04/caffe2%EC%99%80-%EB%94%A5-%EB%9F%AC%EB%8B%9D-%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC/</link><pubDate>Tue, 25 Apr 2017 15:23:34 +0900</pubDate><guid>http://rosinality.github.io/2017/04/caffe2%EC%99%80-%EB%94%A5-%EB%9F%AC%EB%8B%9D-%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC/</guid><description>몰랐는데 Caffe2도 페이스북 쪽에서 기여한 모양(https://caffe2.ai 밑에 Facebook Open Source라고 큼직하게 박혀 있음.) PyTorch</description></item><item><title>머신 러닝과 수학, 그리고 머신 러닝에 입문하기</title><link>http://rosinality.github.io/2017/04/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EC%88%98%ED%95%99-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EC%97%90-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0/</link><pubDate>Tue, 25 Apr 2017 15:20:18 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EC%88%98%ED%95%99-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EC%97%90-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0/</guid><description>데이터 분석에 무슨 종류의 수학이 필요한가 생각해보면, 미적분이나 선형대수는 기본이라고 할 수 있고. 실해석, 측도론, 확률론/확률과정론, 함수해석, 그리고</description></item><item><title>분석사회학과 인과적 설명</title><link>http://rosinality.github.io/2017/04/%EB%B6%84%EC%84%9D%EC%82%AC%ED%9A%8C%ED%95%99%EA%B3%BC-%EC%9D%B8%EA%B3%BC%EC%A0%81-%EC%84%A4%EB%AA%85/</link><pubDate>Fri, 21 Apr 2017 16:23:15 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B6%84%EC%84%9D%EC%82%AC%ED%9A%8C%ED%95%99%EA%B3%BC-%EC%9D%B8%EA%B3%BC%EC%A0%81-%EC%84%A4%EB%AA%85/</guid><description>사회학하면 어쩐지 거대한 이론과 담론들로 대표되는 것 같다. 실제로 그게 주요한 사회학의 전통이라고 해도 크게 틀리지는 않을 것이다. 그와는 좀 다른 전통이 있는데 그</description></item><item><title>K값 논란</title><link>http://rosinality.github.io/2017/04/k%EA%B0%92-%EB%85%BC%EB%9E%80/</link><pubDate>Fri, 21 Apr 2017 16:23:04 +0900</pubDate><guid>http://rosinality.github.io/2017/04/k%EA%B0%92-%EB%85%BC%EB%9E%80/</guid><description>노령인구 비율과 미분류비율로 그린 산포도인데 상당히 강한 선형입니다. 툭 튀어나와있는 점은 세종시인데 세종시는 아웃라이어라고 볼 수 있겠죠. 그래서 분석에서 일</description></item><item><title>모래더미 모형</title><link>http://rosinality.github.io/2017/04/%EB%AA%A8%EB%9E%98%EB%8D%94%EB%AF%B8-%EB%AA%A8%ED%98%95/</link><pubDate>Fri, 21 Apr 2017 16:22:52 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%AA%A8%EB%9E%98%EB%8D%94%EB%AF%B8-%EB%AA%A8%ED%98%95/</guid><description>모래더미 모형이라고 마크 뷰캐넌이 자주 써먹는 모형이 있다. 복잡한 모형은 아니다. 평평한 평면 위 어느 한 곳에 모래를 하나 떨어뜨린다고 하자. 똑같은 곳에 모래가 또</description></item><item><title>빅 데이터</title><link>http://rosinality.github.io/2017/04/%EB%B9%85-%EB%8D%B0%EC%9D%B4%ED%84%B0/</link><pubDate>Fri, 21 Apr 2017 16:21:44 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B9%85-%EB%8D%B0%EC%9D%B4%ED%84%B0/</guid><description>잘 아는 것은 아니지만 빅 데이터가 뭔가 생각해보면&amp;hellip;빅 데이터라고 할 때 데이터를 분석하는 방법 자체는 다 기존의 통계, 머신 러닝, IR에서 왔다고 할</description></item><item><title>배치 정규화 2</title><link>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/</link><pubDate>Fri, 21 Apr 2017 16:18:05 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/</guid><description>batch normalization의 문제 의식은 뉴럴넷에서 하나의 레이어의 출력은 이전의 레이어의 출력에 의해 영향을 받기에, 깊은 뉴럴넷에서는 이런 &amp;ldquo</description></item><item><title>비틀즈가 성공한 이유</title><link>http://rosinality.github.io/2017/04/%EB%B9%84%ED%8B%80%EC%A6%88%EA%B0%80-%EC%84%B1%EA%B3%B5%ED%95%9C-%EC%9D%B4%EC%9C%A0/</link><pubDate>Fri, 21 Apr 2017 16:13:20 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B9%84%ED%8B%80%EC%A6%88%EA%B0%80-%EC%84%B1%EA%B3%B5%ED%95%9C-%EC%9D%B4%EC%9C%A0/</guid><description>Watts가 Turco &amp;amp; Zuckerman이 쓴 비판에 대한 대응으로 쓴 Response to Turco and Zuckerman&amp;rsquo;s &amp;ldquo;Versthen for Sociology&amp;quot;라는 글에서 나온 예시 중에 이런 게 있음. 비틀</description></item><item><title>딥 러닝과 표 형태의 데이터</title><link>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/</link><pubDate>Fri, 21 Apr 2017 16:11:42 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/</guid><description>전통적 통계적 모델링의 대상인 표 형태의 데이터tabular data에 대해서는 딥 러닝이 힘을 못 쓴다(?)는 말을 흔히 한다. 사실 이건 딥 러닝이 이미지나 텍스</description></item><item><title>경험적 위험 최소화</title><link>http://rosinality.github.io/2017/04/%EA%B2%BD%ED%97%98%EC%A0%81-%EC%9C%84%ED%97%98-%EC%B5%9C%EC%86%8C%ED%99%94/</link><pubDate>Fri, 21 Apr 2017 16:02:37 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EA%B2%BD%ED%97%98%EC%A0%81-%EC%9C%84%ED%97%98-%EC%B5%9C%EC%86%8C%ED%99%94/</guid><description>Breiman이 Statistical Modeling: The Two Cultures에서 보여줬던 것처럼 통계적 모델링에는 대체로 두 가지 경로가 있다. 하나는 데이터의 생성 과정을 기술하여 데이터를 모</description></item><item><title>과학의 조건</title><link>http://rosinality.github.io/2017/04/%EA%B3%BC%ED%95%99%EC%9D%98-%EC%A1%B0%EA%B1%B4/</link><pubDate>Fri, 21 Apr 2017 15:59:53 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EA%B3%BC%ED%95%99%EC%9D%98-%EC%A1%B0%EA%B1%B4/</guid><description>과학의 충분조건을 제안하기는 어려운 일이지만 과학의 필요조건을 선정하는 것은 쉽지 않을까? 예측을 포함하는 진술을 하고 그 진술의 예측 능력을 검증하는 프로세스가</description></item><item><title>과학적 진술의 조건</title><link>http://rosinality.github.io/2017/04/%EA%B3%BC%ED%95%99%EC%A0%81-%EC%A7%84%EC%88%A0%EC%9D%98-%EC%A1%B0%EA%B1%B4/</link><pubDate>Fri, 21 Apr 2017 15:56:58 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EA%B3%BC%ED%95%99%EC%A0%81-%EC%A7%84%EC%88%A0%EC%9D%98-%EC%A1%B0%EA%B1%B4/</guid><description>어떤 진술이 과학적인가를 논하는 것보다는 많은 경우에 그렇듯 과학적이지 않은 진술의 조건을 찾는 것이 더 수월할 것이다. 즉 충분조건을 찾기보단 필요조건을 찾는 쪽이</description></item><item><title>행위자 기반 모형과 하이퍼파라미터 최적화</title><link>http://rosinality.github.io/2017/04/%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95%EA%B3%BC-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%B5%9C%EC%A0%81%ED%99%94/</link><pubDate>Fri, 21 Apr 2017 12:24:32 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95%EA%B3%BC-%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%B5%9C%EC%A0%81%ED%99%94/</guid><description>모형의 파라미터는 모형의 학습 과정에서 결정된다. 그렇다면 파라미터에 영향을 미치는 하이퍼파라미터는? 그건 사람이 결정해야 할 문제다. 이걸 잘 정하는 것, 혹은</description></item><item><title>Wasserstein 거리</title><link>http://rosinality.github.io/2017/04/wasserstein-%EA%B1%B0%EB%A6%AC/</link><pubDate>Fri, 21 Apr 2017 11:12:59 +0900</pubDate><guid>http://rosinality.github.io/2017/04/wasserstein-%EA%B1%B0%EB%A6%AC/</guid><description>지금 시점에서는 나온지 좀 되긴 했지만 여전히 재미있는 Wasserstein GAN에 대해서 정리해본다. 뉴럴넷이라는 측면에서도 재미있지만 나오는 수학도 재미있다. Read-through: Wasserstein GAN과</description></item><item><title>상식과 사회학적 설명 2</title><link>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85-2/</link><pubDate>Fri, 21 Apr 2017 11:03:26 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85-2/</guid><description>와츠의 상식과 사회학적 설명은 개인적으로 정말 좋아하는 논문이라 이전에 잠깐 요약을 했었다. 요약을 더 요약하자면 와츠의 논지는 다음과 같다. 사회학 이론들은 대부</description></item><item><title>비선형 모형의 해석가능성</title><link>http://rosinality.github.io/2017/04/%EB%B9%84%EC%84%A0%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B0%80%EB%8A%A5%EC%84%B1/</link><pubDate>Fri, 21 Apr 2017 10:51:13 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B9%84%EC%84%A0%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B0%80%EB%8A%A5%EC%84%B1/</guid><description>해석하기 어려운 블랙박스 모형이라고 하면 대충 어떤 것이 있을까? 아마 대표적인 것이 트리 앙상블일 것이다. 그런데 트리 앙상블이 왜 해석하기 어려운가? 반대로 해석</description></item><item><title>클러스터링과 매니폴드</title><link>http://rosinality.github.io/2017/04/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81%EA%B3%BC-%EB%A7%A4%EB%8B%88%ED%8F%B4%EB%93%9C/</link><pubDate>Fri, 21 Apr 2017 10:40:59 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81%EA%B3%BC-%EB%A7%A4%EB%8B%88%ED%8F%B4%EB%93%9C/</guid><description>클러스터링 알고리즘으로 가장 유명한 것은 K-Means일 것이다. K-Means의 문제점은 1. 클러스터의 숫자를 미리 알아야 한다는 것과 2. 클러스터가 구형이</description></item><item><title>베이지안 뉴럴 네트워크</title><link>http://rosinality.github.io/2017/04/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EB%89%B4%EB%9F%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/</link><pubDate>Fri, 21 Apr 2017 10:35:56 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EB%89%B4%EB%9F%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/</guid><description>https://twitter.com/ylecun/status/764033716276428800 인상적인 성능을 내는 뉴럴넷 모형을 학습하려면 (연산능력의 증가와는 별개로) 늘 2-3주는 필요하다는 이야기가 있다. 수백 개의 GPU를 쓰면 모형도 그만큼 복</description></item><item><title>인과관계와 비모수 모형</title><link>http://rosinality.github.io/2017/04/%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84%EC%99%80-%EB%B9%84%EB%AA%A8%EC%88%98-%EB%AA%A8%ED%98%95/</link><pubDate>Fri, 21 Apr 2017 10:29:01 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84%EC%99%80-%EB%B9%84%EB%AA%A8%EC%88%98-%EB%AA%A8%ED%98%95/</guid><description>인과 그래프 모형 같은 경우는 전형적인 데이터의 생성 과정을 기술하는 모형이지만 동시에 알고리즘적 모형에서 흔히 발견할 수 있는 비모수적 함수 추정이 결합되어 있다.</description></item><item><title>강화 학습과 행위자 기반 모형</title><link>http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/</link><pubDate>Fri, 21 Apr 2017 09:08:57 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/</guid><description>https://deepmind.com/blog/understanding-agent-cooperation/ 최근에 인공지능에 승부욕이 있다느니 혹은 공격성을 보였다느니 하는 식으로 소개된 딥마인드의 연구다. 사실 연구의 핵심은 두 행위자들을 강화학습으로 훈련시켜서</description></item><item><title>프로그래밍 언어와 재현 가능성</title><link>http://rosinality.github.io/2017/04/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EC%96%B8%EC%96%B4%EC%99%80-%EC%9E%AC%ED%98%84-%EA%B0%80%EB%8A%A5%EC%84%B1/</link><pubDate>Fri, 21 Apr 2017 01:18:21 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EC%96%B8%EC%96%B4%EC%99%80-%EC%9E%AC%ED%98%84-%EA%B0%80%EB%8A%A5%EC%84%B1/</guid><description>SPSS와 같은 GUI가 아니라 R과 같은 프로그래밍 언어를 써야 하는 이유가 있는가? (물론 SPSS도 프로그래밍이 가능한 것으로 알고 있다) 라고 하면, 물론 프</description></item><item><title>딥 러닝 모형의 해석</title><link>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/</link><pubDate>Fri, 21 Apr 2017 01:16:36 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/</guid><description>딥 러닝은 이론적 근거가 부족하고 해석이 어렵다는 등등의 평가를 흔히 받는다. 이건 통계학쪽 뿐만 아니라 머신 러닝 커뮤니티쪽에서도 (과거에는) 마찬가지였던 모양</description></item><item><title>4차 산업 혁명</title><link>http://rosinality.github.io/2017/04/4%EC%B0%A8-%EC%82%B0%EC%97%85-%ED%98%81%EB%AA%85/</link><pubDate>Fri, 21 Apr 2017 01:15:36 +0900</pubDate><guid>http://rosinality.github.io/2017/04/4%EC%B0%A8-%EC%82%B0%EC%97%85-%ED%98%81%EB%AA%85/</guid><description>4차 산업혁명이라는 게 구체적으로 어떤 건가 싶어서 찾아봤는데 대충 가장 중요한 문제의식은 인공지능이나 로보틱스 등으로 인해서 산업 현장에서 급진적인 자동화가 일</description></item><item><title>인과적 구조</title><link>http://rosinality.github.io/2017/04/%EC%9D%B8%EA%B3%BC%EC%A0%81-%EA%B5%AC%EC%A1%B0/</link><pubDate>Fri, 21 Apr 2017 01:09:27 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%9D%B8%EA%B3%BC%EC%A0%81-%EA%B5%AC%EC%A1%B0/</guid><description>유디 펄(Judea Pearl)이 어디 초청받아 강연을 하러 가보니까 강연을 주최한 기관 이름이 데이터 사이언스 및 엔지니어링 연구소였다고 한다. 유디 펄이 이게</description></item><item><title>잔차항</title><link>http://rosinality.github.io/2017/04/%EC%9E%94%EC%B0%A8%ED%95%AD/</link><pubDate>Fri, 21 Apr 2017 01:06:24 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%9E%94%EC%B0%A8%ED%95%AD/</guid><description>https://www.theguardian.com/politics/2017/jan/19/crisis-of-statistics-big-data-democracy 예컨대 회귀분석에서 어떤 요인들은 은연중에 오차항으로 취급된다. GDP 등이 중요한 예측 혹은 응답 변수로 취급된다면 이 글에서 제안하는 것과 같이 지역 등등의 요인들</description></item><item><title>배치 정규화</title><link>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94/</link><pubDate>Fri, 21 Apr 2017 01:05:02 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94/</guid><description>배치 정규화는 엄청나게 효과적인 방법이지만 또 사람들이 그렇게 우아한 방법이라고 생각하지 않는 방법이기도 한 듯 하다. 생각해보면 평균을 미니 배치로 추정한다는 것</description></item><item><title>강한 사전 분포</title><link>http://rosinality.github.io/2017/04/%EA%B0%95%ED%95%9C-%EC%82%AC%EC%A0%84-%EB%B6%84%ED%8F%AC/</link><pubDate>Fri, 21 Apr 2017 01:03:34 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EA%B0%95%ED%95%9C-%EC%82%AC%EC%A0%84-%EB%B6%84%ED%8F%AC/</guid><description>균일 사전 분포 같은 정보가 없는 분포가 아닌 정보가 있는 사전 분포(이하 강한 사전 분포)를 사용한다고 할 때 왜 생리적인(?) 거부감 같은 것이 생기는 것일까? 그건 아</description></item><item><title>중도층 공략 전략</title><link>http://rosinality.github.io/2017/04/%EC%A4%91%EB%8F%84%EC%B8%B5-%EA%B3%B5%EB%9E%B5-%EC%A0%84%EB%9E%B5/</link><pubDate>Thu, 20 Apr 2017 21:45:43 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%A4%91%EB%8F%84%EC%B8%B5-%EA%B3%B5%EB%9E%B5-%EC%A0%84%EB%9E%B5/</guid><description>중도층 공략은 매 선거에서 양 정당이 들고나오는 전략인 듯 싶다. 과연 중도층 공략이 얼마나 효과적일 수 있을 것인가에 대해서 좀 정리해보고 싶어졌다. 일단 어떤 지역구</description></item><item><title>일베와 두부</title><link>http://rosinality.github.io/2017/04/%EC%9D%BC%EB%B2%A0%EC%99%80-%EB%91%90%EB%B6%80/</link><pubDate>Thu, 20 Apr 2017 21:37:20 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%9D%BC%EB%B2%A0%EC%99%80-%EB%91%90%EB%B6%80/</guid><description>잘 아는 배우는 아니지만 얼핏 TV에서 본 듯한 배우가 돌바닥 사진을 뒤집어놓고 두부 심부름을 운운한 것 때문에 열성 일베 유저로 몰린 것 같다. 그 배우가 일베러인지 아닌</description></item><item><title>p-값</title><link>http://rosinality.github.io/2017/04/p-%EA%B0%92/</link><pubDate>Thu, 20 Apr 2017 21:35:01 +0900</pubDate><guid>http://rosinality.github.io/2017/04/p-%EA%B0%92/</guid><description>p값과 관련한 문제에는 방법적인 서투름의 문제를 넘어 (물론 방법론적인 문제도 많지만) 연구자들이 다루는 변수들의 설명력은 약하고 데이터의 양은 부족하다는 점에</description></item><item><title>Sequence to Sequence 모형</title><link>http://rosinality.github.io/2017/04/sequence-to-sequence-%EB%AA%A8%ED%98%95/</link><pubDate>Thu, 20 Apr 2017 21:16:13 +0900</pubDate><guid>http://rosinality.github.io/2017/04/sequence-to-sequence-%EB%AA%A8%ED%98%95/</guid><description>기본적인 뉴럴 네트워크는 함수 $f(x)$를 근사하는 것이라고 보면 된다. 입력을 원하는 출력으로 매핑하는 것이다. 예컨대 입력이 이미지라면 입력 이미지가 개인</description></item><item><title>상식과 사회학적 설명</title><link>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85/</link><pubDate>Thu, 20 Apr 2017 20:47:50 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85/</guid><description>와츠-스트로가츠 모형으로 유명한 와츠의 Common Sense and Sociological Explanations (2014)을 아래에 정리했다. 사회학적 설명, 혹은 보다 넓게 말하자면 사회 혹은 사회현상에 대해서 우리가</description></item><item><title>사회학은 과학인가?</title><link>http://rosinality.github.io/2017/04/%EC%82%AC%ED%9A%8C%ED%95%99%EC%9D%80-%EA%B3%BC%ED%95%99%EC%9D%B8%EA%B0%80/</link><pubDate>Thu, 20 Apr 2017 14:04:42 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%82%AC%ED%9A%8C%ED%95%99%EC%9D%80-%EA%B3%BC%ED%95%99%EC%9D%B8%EA%B0%80/</guid><description>사회학은 과학인가? 사회학이 과학이 아니라고, 혹은 사회학은 과학이 될 수 없다고 주장하는 경우 중에는 자연과학의 과학성을 치켜세우면서 애매하고 복잡한 대상인 사</description></item><item><title>신호 탐지 이론</title><link>http://rosinality.github.io/2017/04/%EC%8B%A0%ED%98%B8-%ED%83%90%EC%A7%80-%EC%9D%B4%EB%A1%A0/</link><pubDate>Thu, 20 Apr 2017 14:04:32 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%8B%A0%ED%98%B8-%ED%83%90%EC%A7%80-%EC%9D%B4%EB%A1%A0/</guid><description>인지심리학에서 매우 중요한 주제 중 하나인 신호탐지이론에 대해서 간단하게 알아보자! 신호탐지이론은 2차 세계대전을 전후해서 레이더 관측병들의 행동을 관찰하면</description></item><item><title>시작</title><link>http://rosinality.github.io/2017/04/%EC%8B%9C%EC%9E%91/</link><pubDate>Thu, 20 Apr 2017 14:04:26 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EC%8B%9C%EC%9E%91/</guid><description>위키를 썼던 것부터도 꽤 오랜 시간이 지났다. 별 문제가 없었더라면 계속 위키를 썼겠지만 서버가 관리가 안 되고 있고, 구 버전의 PHP에 구 버전의 도쿠위키를 계속 쓰다</description></item><item><title/><link>http://rosinality.github.io/1/01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://rosinality.github.io/1/01/</guid><description>그 외, 소소한 생각들 이렇게 나열하니 무언가 대단한 가르침이나 격언인 것 같다. 실은 그저 내 경험을 간결하게 요약해보려는 시도일 뿐이다. 고작해야 $N = 1$일 뿐이다.
The Bitter Lesson http://www.incompleteideas.net/IncIdeas/BitterLesson.html (한국어 역: https://newsight.tistory.com/302)
혹시 읽어보지 않은 이들이 있다면 꼭 읽어보라고 권하고 싶다. The Bitter Lesson은 AI/ML을 통틀어 논한 것이지만 딥 러닝에 국한해서도 동일하게 적용되는 것처럼 보인다. 단순하고, 학습과 탐색에 기반하고, 더 큰 모델, 더 많은 데이터, 더 강력한 연산력에 기반한 방법이야말로 더 장기적으로 성공적이며, 더 유지보수하기 쉽다.</description></item><item><title>Learning rate 튜닝</title><link>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</guid><description>Learning rate는 Ian Goodfellow의 Deep Learning 책에도 나오듯 가장 중요한 하이퍼파라미터라고 할 수 있다. 또 그만큼 중요한 하이퍼파라미터가 바로 learning rate를 어떻</description></item></channel></rss>