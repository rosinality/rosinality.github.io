<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sorta informative on Nondifferentiable Log</title>
    <link>http://rosinality.github.io/categories/sorta-informative/</link>
    <description>Recent content in sorta informative on Nondifferentiable Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <lastBuildDate>Sat, 22 May 2021 20:27:53 +0900</lastBuildDate>
    
	<atom:link href="http://rosinality.github.io/categories/sorta-informative/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>머신 러닝 시스템에서 설정 관리하기</title>
      <link>http://rosinality.github.io/2021/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%97%90%EC%84%9C-%EC%84%A4%EC%A0%95-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/</link>
      <pubDate>Sat, 22 May 2021 20:27:53 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2021/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%97%90%EC%84%9C-%EC%84%A4%EC%A0%95-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/</guid>
      <description>머신 러닝 코드, 특히 실험적 목적이 강한 코드에서 가장 중요한 문제 중 하나가 설정을 관리하는 것이라고 본다. 머신 러닝 모델에는 수많은 하이퍼파라미터가 존재하고 그</description>
    </item>
    
    <item>
      <title>머신러닝 파이프라인 만들기</title>
      <link>http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/</link>
      <pubDate>Wed, 14 Aug 2019 13:26:39 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/</guid>
      <description>딥 러닝이 유행하기 시작할 무렵 딥 러닝의 장점으로 나왔던 것이 특징을 추출하는 알고리즘(Feature extractor)을 데이터를 통해 학습한다는 것이었</description>
    </item>
    
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
      <link>http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</link>
      <pubDate>Tue, 16 Oct 2018 10:03:59 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</guid>
      <description>프리트레이닝과 전이학습 모델을 프리트레이닝하는 것이, 혹은 프리트레이닝된 모델이 모듈로 쓰는 것이 성능에 큰 영향을 미칠 수 있다는 건 너무나 잘 알려진 사실이다.</description>
    </item>
    
    <item>
      <title>인과관계를 어떻게 밝힐 수 있는가</title>
      <link>http://rosinality.github.io/2018/08/%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%B0%9D%ED%9E%90-%EC%88%98-%EC%9E%88%EB%8A%94%EA%B0%80/</link>
      <pubDate>Sun, 26 Aug 2018 22:41:37 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2018/08/%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%B0%9D%ED%9E%90-%EC%88%98-%EC%9E%88%EB%8A%94%EA%B0%80/</guid>
      <description>우선 인과관계라는 것이 대체 무엇인가? 인과관계라는 것을 명확하게 정의하는 것은 복잡한 철학적 문제다. 그러나 실용적인 목적으로는 단순한 정의를 사용할 수 있다.</description>
    </item>
    
    <item>
      <title>특징 추출(Feature Extraction)과 딥 러닝</title>
      <link>http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/</link>
      <pubDate>Mon, 08 May 2017 22:41:37 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/</guid>
      <description>https://sinews.siam.org/Details-Page/deep-deep-trouble 뉴럴넷 연구를 하던 사람들이 오랜 겨울을 지나왔던 것처럼 이미지 처리에서, 이젠 전통적인 방법이라고 불리는 방법들을 연구하던 사람들의 고민이 깊은 모양이다. 뉴</description>
    </item>
    
    <item>
      <title>머신 러닝 모델을 만들기</title>
      <link>http://rosinality.github.io/2017/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EB%A7%8C%EB%93%A4%EA%B8%B0/</link>
      <pubDate>Mon, 08 May 2017 22:31:57 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/05/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EB%A7%8C%EB%93%A4%EA%B8%B0/</guid>
      <description>머신 러닝을 하는데 성능이 잘 안 나온다는 질문이 종종 보여서, 모델 성능 평가의 왕도를 소개해봄. 사실 Deep Learning book의 챕터 11에 나오는 내용. 전처리를 하고 데이터</description>
    </item>
    
    <item>
      <title>2017 대선 여론조사 분석</title>
      <link>http://rosinality.github.io/2017/05/2017-%EB%8C%80%EC%84%A0-%EC%97%AC%EB%A1%A0%EC%A1%B0%EC%82%AC-%EB%B6%84%EC%84%9D/</link>
      <pubDate>Mon, 01 May 2017 03:05:27 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/05/2017-%EB%8C%80%EC%84%A0-%EC%97%AC%EB%A1%A0%EC%A1%B0%EC%82%AC-%EB%B6%84%EC%84%9D/</guid>
      <description>우선, 2012년에는 우선 2012년 여론조사와 실제 대선 결과를 비교해보면 다음과 같다. 여론조사 결과의 경우 합이 100이 되도록 각 값을 값의 합으로 나눴다.</description>
    </item>
    
    <item>
      <title>Caffe2에 대해</title>
      <link>http://rosinality.github.io/2017/04/caffe2%EC%97%90-%EB%8C%80%ED%95%B4/</link>
      <pubDate>Tue, 25 Apr 2017 15:28:52 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/caffe2%EC%97%90-%EB%8C%80%ED%95%B4/</guid>
      <description>PyTorch랑 장기적으로 백엔드를 통합한다니까 관심이 생겨서 Caffe2를 좀 들여다봤음. Caffe를 써본 적은 없음. protocol buffer 파일을 만들어서 돌린다는 것</description>
    </item>
    
    <item>
      <title>분석사회학과 인과적 설명</title>
      <link>http://rosinality.github.io/2017/04/%EB%B6%84%EC%84%9D%EC%82%AC%ED%9A%8C%ED%95%99%EA%B3%BC-%EC%9D%B8%EA%B3%BC%EC%A0%81-%EC%84%A4%EB%AA%85/</link>
      <pubDate>Fri, 21 Apr 2017 16:23:15 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EB%B6%84%EC%84%9D%EC%82%AC%ED%9A%8C%ED%95%99%EA%B3%BC-%EC%9D%B8%EA%B3%BC%EC%A0%81-%EC%84%A4%EB%AA%85/</guid>
      <description>사회학하면 어쩐지 거대한 이론과 담론들로 대표되는 것 같다. 실제로 그게 주요한 사회학의 전통이라고 해도 크게 틀리지는 않을 것이다. 그와는 좀 다른 전통이 있는데 그</description>
    </item>
    
    <item>
      <title>모래더미 모형</title>
      <link>http://rosinality.github.io/2017/04/%EB%AA%A8%EB%9E%98%EB%8D%94%EB%AF%B8-%EB%AA%A8%ED%98%95/</link>
      <pubDate>Fri, 21 Apr 2017 16:22:52 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EB%AA%A8%EB%9E%98%EB%8D%94%EB%AF%B8-%EB%AA%A8%ED%98%95/</guid>
      <description>모래더미 모형이라고 마크 뷰캐넌이 자주 써먹는 모형이 있다. 복잡한 모형은 아니다. 평평한 평면 위 어느 한 곳에 모래를 하나 떨어뜨린다고 하자. 똑같은 곳에 모래가 또</description>
    </item>
    
    <item>
      <title>배치 정규화 2</title>
      <link>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/</link>
      <pubDate>Fri, 21 Apr 2017 16:18:05 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/</guid>
      <description>batch normalization의 문제 의식은 뉴럴넷에서 하나의 레이어의 출력은 이전의 레이어의 출력에 의해 영향을 받기에, 깊은 뉴럴넷에서는 이런 &amp;ldquo</description>
    </item>
    
    <item>
      <title>비틀즈가 성공한 이유</title>
      <link>http://rosinality.github.io/2017/04/%EB%B9%84%ED%8B%80%EC%A6%88%EA%B0%80-%EC%84%B1%EA%B3%B5%ED%95%9C-%EC%9D%B4%EC%9C%A0/</link>
      <pubDate>Fri, 21 Apr 2017 16:13:20 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EB%B9%84%ED%8B%80%EC%A6%88%EA%B0%80-%EC%84%B1%EA%B3%B5%ED%95%9C-%EC%9D%B4%EC%9C%A0/</guid>
      <description>Watts가 Turco &amp;amp; Zuckerman이 쓴 비판에 대한 대응으로 쓴 Response to Turco and Zuckerman&amp;rsquo;s &amp;ldquo;Versthen for Sociology&amp;quot;라는 글에서 나온 예시 중에 이런 게 있음. 비틀</description>
    </item>
    
    <item>
      <title>경험적 위험 최소화</title>
      <link>http://rosinality.github.io/2017/04/%EA%B2%BD%ED%97%98%EC%A0%81-%EC%9C%84%ED%97%98-%EC%B5%9C%EC%86%8C%ED%99%94/</link>
      <pubDate>Fri, 21 Apr 2017 16:02:37 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EA%B2%BD%ED%97%98%EC%A0%81-%EC%9C%84%ED%97%98-%EC%B5%9C%EC%86%8C%ED%99%94/</guid>
      <description>Breiman이 Statistical Modeling: The Two Cultures에서 보여줬던 것처럼 통계적 모델링에는 대체로 두 가지 경로가 있다. 하나는 데이터의 생성 과정을 기술하여 데이터를 모</description>
    </item>
    
    <item>
      <title>Wasserstein 거리</title>
      <link>http://rosinality.github.io/2017/04/wasserstein-%EA%B1%B0%EB%A6%AC/</link>
      <pubDate>Fri, 21 Apr 2017 11:12:59 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/wasserstein-%EA%B1%B0%EB%A6%AC/</guid>
      <description>지금 시점에서는 나온지 좀 되긴 했지만 여전히 재미있는 Wasserstein GAN에 대해서 정리해본다. 뉴럴넷이라는 측면에서도 재미있지만 나오는 수학도 재미있다. Read-through: Wasserstein GAN과</description>
    </item>
    
    <item>
      <title>상식과 사회학적 설명 2</title>
      <link>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85-2/</link>
      <pubDate>Fri, 21 Apr 2017 11:03:26 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85-2/</guid>
      <description>와츠의 상식과 사회학적 설명은 개인적으로 정말 좋아하는 논문이라 이전에 잠깐 요약을 했었다. 요약을 더 요약하자면 와츠의 논지는 다음과 같다. 사회학 이론들은 대부</description>
    </item>
    
    <item>
      <title>비선형 모형의 해석가능성</title>
      <link>http://rosinality.github.io/2017/04/%EB%B9%84%EC%84%A0%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B0%80%EB%8A%A5%EC%84%B1/</link>
      <pubDate>Fri, 21 Apr 2017 10:51:13 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EB%B9%84%EC%84%A0%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B0%80%EB%8A%A5%EC%84%B1/</guid>
      <description>해석하기 어려운 블랙박스 모형이라고 하면 대충 어떤 것이 있을까? 아마 대표적인 것이 트리 앙상블일 것이다. 그런데 트리 앙상블이 왜 해석하기 어려운가? 반대로 해석</description>
    </item>
    
    <item>
      <title>클러스터링과 매니폴드</title>
      <link>http://rosinality.github.io/2017/04/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81%EA%B3%BC-%EB%A7%A4%EB%8B%88%ED%8F%B4%EB%93%9C/</link>
      <pubDate>Fri, 21 Apr 2017 10:40:59 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81%EA%B3%BC-%EB%A7%A4%EB%8B%88%ED%8F%B4%EB%93%9C/</guid>
      <description>클러스터링 알고리즘으로 가장 유명한 것은 K-Means일 것이다. K-Means의 문제점은 1. 클러스터의 숫자를 미리 알아야 한다는 것과 2. 클러스터가 구형이</description>
    </item>
    
    <item>
      <title>배치 정규화</title>
      <link>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94/</link>
      <pubDate>Fri, 21 Apr 2017 01:05:02 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94/</guid>
      <description>배치 정규화는 엄청나게 효과적인 방법이지만 또 사람들이 그렇게 우아한 방법이라고 생각하지 않는 방법이기도 한 듯 하다. 생각해보면 평균을 미니 배치로 추정한다는 것</description>
    </item>
    
    <item>
      <title>Sequence to Sequence 모형</title>
      <link>http://rosinality.github.io/2017/04/sequence-to-sequence-%EB%AA%A8%ED%98%95/</link>
      <pubDate>Thu, 20 Apr 2017 21:16:13 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/sequence-to-sequence-%EB%AA%A8%ED%98%95/</guid>
      <description>기본적인 뉴럴 네트워크는 함수 $f(x)$를 근사하는 것이라고 보면 된다. 입력을 원하는 출력으로 매핑하는 것이다. 예컨대 입력이 이미지라면 입력 이미지가 개인</description>
    </item>
    
    <item>
      <title>상식과 사회학적 설명</title>
      <link>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85/</link>
      <pubDate>Thu, 20 Apr 2017 20:47:50 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EC%83%81%EC%8B%9D%EA%B3%BC-%EC%82%AC%ED%9A%8C%ED%95%99%EC%A0%81-%EC%84%A4%EB%AA%85/</guid>
      <description>와츠-스트로가츠 모형으로 유명한 와츠의 Common Sense and Sociological Explanations (2014)을 아래에 정리했다. 사회학적 설명, 혹은 보다 넓게 말하자면 사회 혹은 사회현상에 대해서 우리가</description>
    </item>
    
    <item>
      <title>신호 탐지 이론</title>
      <link>http://rosinality.github.io/2017/04/%EC%8B%A0%ED%98%B8-%ED%83%90%EC%A7%80-%EC%9D%B4%EB%A1%A0/</link>
      <pubDate>Thu, 20 Apr 2017 14:04:32 +0900</pubDate>
      
      <guid>http://rosinality.github.io/2017/04/%EC%8B%A0%ED%98%B8-%ED%83%90%EC%A7%80-%EC%9D%B4%EB%A1%A0/</guid>
      <description>인지심리학에서 매우 중요한 주제 중 하나인 신호탐지이론에 대해서 간단하게 알아보자! 신호탐지이론은 2차 세계대전을 전후해서 레이더 관측병들의 행동을 관찰하면</description>
    </item>
    
    <item>
      <title>Learning rate 튜닝</title>
      <link>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</guid>
      <description>Learning rate는 Ian Goodfellow의 Deep Learning 책에도 나오듯 가장 중요한 하이퍼파라미터라고 할 수 있다. 또 그만큼 중요한 하이퍼파라미터가 바로 learning rate를 어떻</description>
    </item>
    
  </channel>
</rss>