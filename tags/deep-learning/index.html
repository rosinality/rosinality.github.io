<!doctype html><html lang=ko-kr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.73.0"><meta name=theme content="Tranquilpeak 0.3.1-BETA"><title>deep learning</title><meta name=author content="Kim Seonghyeon"><meta name=keywords content=",machine learning,deep learning,social science"><link rel=icon href=/favicon.png><link rel=alternate type=application/rss+xml title=RSS href=http://rosinality.github.io/tags/deep-learning/index.xml><meta name=description content><meta property="og:description" content><meta property="og:type" content="blog"><meta property="og:title" content="deep learning"><meta property="og:url" content="/tags/deep-learning/"><meta property="og:site_name" content="Nondifferentiable Log"><meta name=twitter:card content="summary"><meta name=twitter:title content="Nondifferentiable Log"><meta name=twitter:description content><meta name=twitter:creator content="@rosinality"><meta property="og:image" content="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=640"><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css><link rel=stylesheet href=/css/style-u6mk0ojoywresbx8iepslrmmhl4stuhrsxuwhkpwrkrx7mryjcaimasnk4pi.min.css></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=/>Nondifferentiable Log</a></div></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=/#about><img class=sidebar-profile-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt></a><h4 class=sidebar-profile-name>Kim Seonghyeon</h4><h5 class=sidebar-profile-bio>Machine learning enthusiast</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/categories><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/tags><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tags</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/archives><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/#about><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>About</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Home</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/rosinality target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><section class="postShorten-group main-content-wrap"><article class="postShorten postShorten--thumbnailimg-top" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><a href=http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/><div class=postShorten-thumbnailimg><img alt itemprop=image src=https://res.cloudinary.com/rosinality/image/upload/c_pad,w_2048/scaling-law/ajyfaru0bjpaq5euvzxs></div></a><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/>Scaling Law, Architecture for Stability and Layer Stacking</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2024-09-11T12:24:59+09:00>September 11, 2024</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta-informative</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about …<br><a href=http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/ class="postShorten-excerpt_link link"></a></p></div></div></article><article class="postShorten postShorten--thumbnailimg-top" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><a href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/><div class=postShorten-thumbnailimg><img alt itemprop=image src=https://res.cloudinary.com/rosinality/image/upload/c_pad,w_2048/v1717458764/prelimiary-exploration/andrew-neel-1-29wyvvLJA-unsplash.jpg></div></a><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/>Preliminary Explorations on UL2 and Second-order Optimizers</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2024-06-04T07:28:30+09:00>June 4, 2024</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta-informative</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And, …<br><a href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/ class="postShorten-excerpt_link link"></a></p></div></div></article><article class="postShorten postShorten--thumbnailimg-top" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><a href=http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/><div class=postShorten-thumbnailimg><img alt itemprop=image src=https://res.cloudinary.com/rosinality/image/upload/c_crop,h_700,w_3000/v1566137009/pipeline.jpg></div></a><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/>머신러닝 파이프라인 만들기</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2019-08-14T13:26:39+09:00>August 14, 2019</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta-informative</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>딥 러닝이 유행하기 시작할 무렵 딥 러닝의 장점으로 나왔던 것이 특징을 추출하는 알고리즘(Feature extractor)을 데이터를 통해 학습한다는 것이었 …<br><a href=http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/ class="postShorten-excerpt_link link"></a></p></div></div></article><article class="postShorten postShorten--thumbnailimg-top" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><a href=http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/><div class=postShorten-thumbnailimg><img alt itemprop=image src=https://res.cloudinary.com/rosinality/image/upload/v1539694498/train_a7myvw.jpg></div></a><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2018-10-16T10:03:59+09:00>October 16, 2018</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta-informative</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>프리트레이닝과 전이학습 모델을 프리트레이닝하는 것이, 혹은 프리트레이닝된 모델이 모듈로 쓰는 것이 성능에 큰 영향을 미칠 수 있다는 건 너무나 잘 알려진 사실이다. …<br><a href=http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/ class="postShorten-excerpt_link link"></a></p></div></div></article><article class="postShorten postShorten--thumbnailimg-left" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/>앞으로 재미있을지도 모르는 주제들</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-11-03T10:03:59+09:00>November 3, 2017</time>
<span></span><a class=category-link href=/categories/thoughts>thoughts</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거 …<br><a href=http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/ class="postShorten-excerpt_link link"></a></p></div></div><a href=http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/><div class=postShorten-thumbnailimg><img alt itemprop=image src=https://res.cloudinary.com/rosinality/image/upload/v1509636468/candle_kgokhm.jpg></div></a></article><article class="postShorten postShorten--thumbnailimg-top" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><a href=http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/><div class=postShorten-thumbnailimg><img alt itemprop=image src=http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/imagenet.jpg></div></a><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/>특징 추출(Feature Extraction)과 딥 러닝</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-05-08T22:41:37+09:00>May 8, 2017</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta informative</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p><a href=https://sinews.siam.org/Details-Page/deep-deep-trouble>https://sinews.siam.org/Details-Page/deep-deep-trouble</a> 뉴럴넷 연구를 하던 사람들이 오랜 겨울을 지나왔던 것처럼 이미지 처리에서, 이젠 전통적인 방법이라고 불리는 방법들을 연구하던 사람들의 고민이 깊은 모양이다. 뉴 …<br><a href=http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/ class="postShorten-excerpt_link link"></a></p></div></div></article><article class="postShorten postShorten--thumbnailimg-top" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><a href=http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/><div class=postShorten-thumbnailimg><img alt itemprop=image src=http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/batches.jpg></div></a><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/>배치 정규화 2</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-04-21T16:18:05+09:00>April 21, 2017</time>
<span></span><a class=category-link href=/categories/sorta-informative>sorta-informative</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>batch normalization의 문제 의식은 뉴럴넷에서 하나의 레이어의 출력은 이전의 레이어의 출력에 의해 영향을 받기에, 깊은 뉴럴넷에서는 이런 &ldquo …<br><a href=http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/ class="postShorten-excerpt_link link"></a></p></div></div></article><article class="postShorten postShorten--thumbnailimg-left" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/>딥 러닝과 표 형태의 데이터</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-04-21T16:11:42+09:00>April 21, 2017</time>
<span></span><a class=category-link href=/categories/thoughts>thoughts</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>전통적 통계적 모델링의 대상인 표 형태의 데이터tabular data에 대해서는 딥 러닝이 힘을 못 쓴다(?)는 말을 흔히 한다. 사실 이건 딥 러닝이 이미지나 텍스 …<br><a href=http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/ class="postShorten-excerpt_link link"></a></p></div></div><a href=http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/><div class=postShorten-thumbnailimg><img alt itemprop=image src=http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/table.jpg></div></a></article><article class="postShorten postShorten--thumbnailimg-left" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/>강화 학습과 행위자 기반 모형</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-04-21T09:08:57+09:00>April 21, 2017</time>
<span></span><a class=category-link href=/categories/thoughts>thoughts</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p><a href=https://deepmind.com/blog/understanding-agent-cooperation/>https://deepmind.com/blog/understanding-agent-cooperation/</a> 최근에 인공지능에 승부욕이 있다느니 혹은 공격성을 보였다느니 하는 식으로 소개된 딥마인드의 연구다. 사실 연구의 핵심은 두 행위자들을 강화학습으로 훈련시켜서 …<br><a href=http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/ class="postShorten-excerpt_link link"></a></p></div></div><a href=http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/><div class=postShorten-thumbnailimg><img alt itemprop=image src=http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/fire.png></div></a></article><article class="postShorten postShorten--thumbnailimg-left" itemscope itemtype=http://schema.org/BlogPosting><div class=postShorten-wrap><div class=postShorten-header><h1 class=postShorten-title itemprop=headline><a class=link-unstyled href=http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/>딥 러닝 모형의 해석</a></h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-04-21T01:16:36+09:00>April 21, 2017</time>
<span></span><a class=category-link href=/categories/thoughts>thoughts</a></div></div><div class=postShorten-excerpt itemprop=articleBody><p>딥 러닝은 이론적 근거가 부족하고 해석이 어렵다는 등등의 평가를 흔히 받는다. 이건 통계학쪽 뿐만 아니라 머신 러닝 커뮤니티쪽에서도 (과거에는) 마찬가지였던 모양 …<br><a href=http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/ class="postShorten-excerpt_link link"></a></p></div></div><a href=http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/><div class=postShorten-thumbnailimg><img alt itemprop=image src=http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/interpret.jpg></div></a></article><div class=pagination-bar><ul class=pagination><li class=pagination-next><a class="btn btn--default btn--small" href=/tags/deep-learning/page/2/><span></span><i class="fa fa-angle-right text-base icon-ml"></i></a></li><li class=pagination-number></li></ul></div></section><footer id=footer class=main-content-wrap><span class=copyrights>&copy; 2024 Kim Seonghyeon.</span></footer></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="//www.gravatar.com/avatar/63eec5e671fc734bde45cd43cc156abc?s=110" alt><h4 id=about-card-name>Kim Seonghyeon</h4><div id=about-card-bio>Machine learning enthusiast</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Graduate student in HCCLab at Seoul National University</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Korea, Republic of</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input type=text id=algolia-search-input name=search class="form-control input--large search-input" placeholder></form></div><div class=modal-body><div class="no-result text-color-light text-center"></div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/post/><h3 class=media-heading>Posts</h3></a><span class=media-meta><span class="media-date text-small">Sep 9, 2024</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/><h3 class=media-heading>Scaling Law, Architecture for Stability and Layer Stacking</h3></a><span class=media-meta><span class="media-date text-small">Sep 9, 2024</span></span><div class="media-content hide-xs font-merryweather">Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/><h3 class=media-heading>Preliminary Explorations on UL2 and Second-order Optimizers</h3></a><span class=media-meta><span class="media-date text-small">Jun 6, 2024</span></span><div class="media-content hide-xs font-merryweather">In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/07/constitutional-ai/><h3 class=media-heading>Constitutional AI</h3></a><span class=media-meta><span class="media-date text-small">Jul 7, 2023</span></span><div class="media-content hide-xs font-merryweather">Helpful & Harmless Agent AI 모델의 정렬(Alignment)이라고 이야기할 때 흔히 나오는 Helpfulness와 Harmlessness는 어떤 의미인가? 이는 정의</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%99%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%83%9D%EC%84%B1-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4/><h3 class=media-heading>이미지와 텍스트 생성 모델에 대해</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">이미지 생성 하면 Style GAN이었던 시절에도 일러스트 생성 등은 오타쿠적 인기가 있는 주제였다. 문제의 Danbooru 데이터셋 같은 경우에도 그 시점에 이미 만들어진 데이터셋이었</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/02/%EC%96%B8%EC%96%B4%EC%9D%98-%EC%86%90%EC%8B%A4-%EC%95%95%EC%B6%95%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/><h3 class=media-heading>언어의 손실 압축에 대하여</h3></a><span class=media-meta><span class="media-date text-small">Feb 2, 2023</span></span><div class="media-content hide-xs font-merryweather">https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web LM을 다음 단어를 예측할 뿐이라거나 학습 데이터를 기억할 뿐이라는 식으로 묘사하는 것은 폄하를 위한 언어이지 LM의 실체나 실제 한계에 대해서 논하기에 적절한 방</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2023/01/ocr-%ED%9A%8C%EA%B3%A0/><h3 class=media-heading>OCR 회고</h3></a><span class=media-meta><span class="media-date text-small">Jan 1, 2023</span></span><div class="media-content hide-xs font-merryweather">타이틀 커버 이미지 출처: https://www.behance.net/gallery/6146939/OCR-A-Poster/modules/152114859 4년 동안 몰두했던 OCR이라는 주제를 마무리하게 되면서 으레 그래왔듯 회고를 남겨본다. 이랬더라면 어땠을까 같은 소소한 소회보다는</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-7/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 7</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-6/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 6</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=http://rosinality.github.io/2022/11/%ED%85%94-%EC%95%84%EB%B9%84%EB%B8%8C%EC%99%80-eccv-2022-%EC%97%AC%ED%96%89%EA%B8%B0-5/><h3 class=media-heading>텔 아비브와 ECCV 2022 여행기 5</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2022</span></span><div class="media-content hide-xs font-merryweather">텔 아비브와 ECCV 2022 여행기 1 텔 아비브와 ECCV 2022 여행기 2 텔 아비브와 ECCV 2022 여행기 3 텔 아비브와 ECCV 2022 여행기 4 텔 아비브와 ECCV 2022 여행기 5 텔 아비브와 ECCV 2022 여행기 6 텔 아비브</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero data-message-one data-message-other>68 posts found</p></div></div></div><div id=cover style=background-image:url(http://res.cloudinary.com/rosinality/image/upload/v1492734059/covers/stair.jpg)></div><script>(function(d){var config={kitId:'vld6lxf',scriptTimeout:3000,async:true},h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='https://use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)})(document);</script><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js></script><script src=//cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js></script><script src=/js/script-wl33z0n6ocaypepiqrazthtivfrliqijej4rq8ek8gvrv1awftmgjuv8k4zc.min.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight').each(function(i,block){var code="";hljs.highlightAuto(block.innerText).value.split(/\r\n|\r|\n/).forEach(function(line){code+="<span class=\"line\">"+line+"</span><br>";});if(code.length>0){block.innerHTML=code;}});$('pre > code').each(function(i,block){$(this).addClass('codeblock');hljs.highlightBlock(block);});});</script></body></html>