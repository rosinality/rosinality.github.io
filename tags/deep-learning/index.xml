<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep learning on Nondifferentiable Log</title><link>http://rosinality.github.io/tags/deep-learning/</link><description>Recent content in deep learning on Nondifferentiable Log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Wed, 14 Aug 2019 13:26:39 +0900</lastBuildDate><atom:link href="http://rosinality.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>머신러닝 파이프라인 만들기</title><link>http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/</link><pubDate>Wed, 14 Aug 2019 13:26:39 +0900</pubDate><guid>http://rosinality.github.io/2019/08/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/</guid><description>딥 러닝이 유행하기 시작할 무렵 딥 러닝의 장점으로 나왔던 것이 특징을 추출하는 알고리즘(Feature extractor)을 데이터를 통해 학습한다는 것이었</description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</link><pubDate>Tue, 16 Oct 2018 10:03:59 +0900</pubDate><guid>http://rosinality.github.io/2018/10/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</guid><description>프리트레이닝과 전이학습 모델을 프리트레이닝하는 것이, 혹은 프리트레이닝된 모델이 모듈로 쓰는 것이 성능에 큰 영향을 미칠 수 있다는 건 너무나 잘 알려진 사실이다.</description></item><item><title>앞으로 재미있을지도 모르는 주제들</title><link>http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/</link><pubDate>Fri, 03 Nov 2017 10:03:59 +0900</pubDate><guid>http://rosinality.github.io/2017/11/%EC%95%9E%EC%9C%BC%EB%A1%9C-%EC%9E%AC%EB%AF%B8%EC%9E%88%EC%9D%84%EC%A7%80%EB%8F%84-%EB%AA%A8%EB%A5%B4%EB%8A%94-%EC%A3%BC%EC%A0%9C%EB%93%A4/</guid><description>LSTM을 대신할 RNN Cell을 설계한다거나 하는 식의 기존의 구조를 개선하는 방안을 고안하는 것은 분명히 중요한 일이기는 하지만 그 자체로는 이전에는 불가능하거</description></item><item><title>특징 추출(Feature Extraction)과 딥 러닝</title><link>http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/</link><pubDate>Mon, 08 May 2017 22:41:37 +0900</pubDate><guid>http://rosinality.github.io/2017/05/%ED%8A%B9%EC%A7%95-%EC%B6%94%EC%B6%9Cfeature-extraction%EA%B3%BC-%EB%94%A5-%EB%9F%AC%EB%8B%9D/</guid><description>https://sinews.siam.org/Details-Page/deep-deep-trouble 뉴럴넷 연구를 하던 사람들이 오랜 겨울을 지나왔던 것처럼 이미지 처리에서, 이젠 전통적인 방법이라고 불리는 방법들을 연구하던 사람들의 고민이 깊은 모양이다. 뉴</description></item><item><title>배치 정규화 2</title><link>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/</link><pubDate>Fri, 21 Apr 2017 16:18:05 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94-2/</guid><description>batch normalization의 문제 의식은 뉴럴넷에서 하나의 레이어의 출력은 이전의 레이어의 출력에 의해 영향을 받기에, 깊은 뉴럴넷에서는 이런 &amp;ldquo</description></item><item><title>딥 러닝과 표 형태의 데이터</title><link>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/</link><pubDate>Fri, 21 Apr 2017 16:11:42 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D%EA%B3%BC-%ED%91%9C-%ED%98%95%ED%83%9C%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0/</guid><description>전통적 통계적 모델링의 대상인 표 형태의 데이터tabular data에 대해서는 딥 러닝이 힘을 못 쓴다(?)는 말을 흔히 한다. 사실 이건 딥 러닝이 이미지나 텍스</description></item><item><title>강화 학습과 행위자 기반 모형</title><link>http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/</link><pubDate>Fri, 21 Apr 2017 09:08:57 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EA%B0%95%ED%99%94-%ED%95%99%EC%8A%B5%EA%B3%BC-%ED%96%89%EC%9C%84%EC%9E%90-%EA%B8%B0%EB%B0%98-%EB%AA%A8%ED%98%95/</guid><description>https://deepmind.com/blog/understanding-agent-cooperation/ 최근에 인공지능에 승부욕이 있다느니 혹은 공격성을 보였다느니 하는 식으로 소개된 딥마인드의 연구다. 사실 연구의 핵심은 두 행위자들을 강화학습으로 훈련시켜서</description></item><item><title>딥 러닝 모형의 해석</title><link>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/</link><pubDate>Fri, 21 Apr 2017 01:16:36 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/</guid><description>딥 러닝은 이론적 근거가 부족하고 해석이 어렵다는 등등의 평가를 흔히 받는다. 이건 통계학쪽 뿐만 아니라 머신 러닝 커뮤니티쪽에서도 (과거에는) 마찬가지였던 모양</description></item><item><title>4차 산업 혁명</title><link>http://rosinality.github.io/2017/04/4%EC%B0%A8-%EC%82%B0%EC%97%85-%ED%98%81%EB%AA%85/</link><pubDate>Fri, 21 Apr 2017 01:15:36 +0900</pubDate><guid>http://rosinality.github.io/2017/04/4%EC%B0%A8-%EC%82%B0%EC%97%85-%ED%98%81%EB%AA%85/</guid><description>4차 산업혁명이라는 게 구체적으로 어떤 건가 싶어서 찾아봤는데 대충 가장 중요한 문제의식은 인공지능이나 로보틱스 등으로 인해서 산업 현장에서 급진적인 자동화가 일</description></item><item><title>배치 정규화</title><link>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94/</link><pubDate>Fri, 21 Apr 2017 01:05:02 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94/</guid><description>배치 정규화는 엄청나게 효과적인 방법이지만 또 사람들이 그렇게 우아한 방법이라고 생각하지 않는 방법이기도 한 듯 하다. 생각해보면 평균을 미니 배치로 추정한다는 것</description></item><item><title>Learning rate 튜닝</title><link>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</guid><description>Learning rate는 Ian Goodfellow의 Deep Learning 책에도 나오듯 가장 중요한 하이퍼파라미터라고 할 수 있다. 또 그만큼 중요한 하이퍼파라미터가 바로 learning rate를 어떻</description></item></channel></rss>