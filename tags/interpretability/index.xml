<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>interpretability on Nondifferentiable Log</title><link>http://rosinality.github.io/tags/interpretability/</link><description>Recent content in interpretability on Nondifferentiable Log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Fri, 21 Apr 2017 10:51:13 +0900</lastBuildDate><atom:link href="http://rosinality.github.io/tags/interpretability/index.xml" rel="self" type="application/rss+xml"/><item><title>비선형 모형의 해석가능성</title><link>http://rosinality.github.io/2017/04/%EB%B9%84%EC%84%A0%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B0%80%EB%8A%A5%EC%84%B1/</link><pubDate>Fri, 21 Apr 2017 10:51:13 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%B9%84%EC%84%A0%ED%98%95-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D%EA%B0%80%EB%8A%A5%EC%84%B1/</guid><description>해석하기 어려운 블랙박스 모형이라고 하면 대충 어떤 것이 있을까? 아마 대표적인 것이 트리 앙상블일 것이다. 그런데 트리 앙상블이 왜 해석하기 어려운가? 반대로 해석</description></item><item><title>딥 러닝 모형의 해석</title><link>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/</link><pubDate>Fri, 21 Apr 2017 01:16:36 +0900</pubDate><guid>http://rosinality.github.io/2017/04/%EB%94%A5-%EB%9F%AC%EB%8B%9D-%EB%AA%A8%ED%98%95%EC%9D%98-%ED%95%B4%EC%84%9D/</guid><description>딥 러닝은 이론적 근거가 부족하고 해석이 어렵다는 등등의 평가를 흔히 받는다. 이건 통계학쪽 뿐만 아니라 머신 러닝 커뮤니티쪽에서도 (과거에는) 마찬가지였던 모양</description></item></channel></rss>