<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>language models on Nondifferentiable Log</title><link>http://rosinality.github.io/tags/language-models/</link><description>Recent content in language models on Nondifferentiable Log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Wed, 11 Sep 2024 12:24:59 +0900</lastBuildDate><atom:link href="http://rosinality.github.io/tags/language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Scaling Law, Architecture for Stability and Layer Stacking</title><link>http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/</link><pubDate>Wed, 11 Sep 2024 12:24:59 +0900</pubDate><guid>http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/</guid><description>Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about</description></item><item><title>Preliminary Explorations on UL2 and Second-order Optimizers</title><link>http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/</link><pubDate>Tue, 04 Jun 2024 07:28:30 +0900</pubDate><guid>http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/</guid><description>In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</description></item></channel></rss>