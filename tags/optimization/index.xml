<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>optimization on Nondifferentiable Log</title><link>http://rosinality.github.io/tags/optimization/</link><description>Recent content in optimization on Nondifferentiable Log</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Wed, 11 Sep 2024 12:24:59 +0900</lastBuildDate><atom:link href="http://rosinality.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>Scaling Law, Architecture for Stability and Layer Stacking</title><link>http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/</link><pubDate>Wed, 11 Sep 2024 12:24:59 +0900</pubDate><guid>http://rosinality.github.io/2024/09/scaling-law-architecture-for-stability-and-layer-stacking/</guid><description>Scaling Law Scaling law is one of the most important findings in LLMs (and neural networks in general) 1. You can make almost all important decisions about training of models with scaling law. For example you can choose model size, number of training steps 2, hyperparameters such as learning rate and batch size 3, learning rate schedules 4, mixture of training datasets 5, etc. So if you are serious about</description></item><item><title>Preliminary Explorations on UL2 and Second-order Optimizers</title><link>http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/</link><pubDate>Tue, 04 Jun 2024 07:28:30 +0900</pubDate><guid>http://rosinality.github.io/2024/06/preliminary-explorations-on-ul2-and-second-order-optimizers/</guid><description>In the field of large language models, the most important recipes to cook the model is not opened to publics. Model architecture itself is quite well-known because many state-of-the-art models are now open weights, and in many cases we find it is a boringly simple vanilla transformers. But for datasets and training objectives it is not well known, and many LLM builders deliberately obfuscates the details of these two. And,</description></item><item><title>Learning rate 튜닝</title><link>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://rosinality.github.io/1/01/learning-rate-%ED%8A%9C%EB%8B%9D/</guid><description>Learning rate는 Ian Goodfellow의 Deep Learning 책에도 나오듯 가장 중요한 하이퍼파라미터라고 할 수 있다. 또 그만큼 중요한 하이퍼파라미터가 바로 learning rate를 어떻</description></item></channel></rss>